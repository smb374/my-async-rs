<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Design and Implementation Detail of my-async</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="intro.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">Prerequisite Knowledge</li><li class="chapter-item expanded "><a href="pre/async_in_rust.html"><strong aria-hidden="true">1.</strong> Asynchronous in Rust</a></li><li class="chapter-item expanded "><a href="pre/overview.html"><strong aria-hidden="true">2.</strong> Overview of an executor's architecture</a></li><li class="chapter-item expanded "><a href="pre/single_thread_executor.html"><strong aria-hidden="true">3.</strong> A minimal single-threaded Future evaluator</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="pre/single_future_handle.html"><strong aria-hidden="true">3.1.</strong> Future handling</a></li><li class="chapter-item expanded "><a href="pre/single_global_storage.html"><strong aria-hidden="true">3.2.</strong> Global Storage</a></li><li class="chapter-item expanded "><a href="pre/single_message_passing.html"><strong aria-hidden="true">3.3.</strong> Message Passing</a></li><li class="chapter-item expanded "><a href="pre/single_executor.html"><strong aria-hidden="true">3.4.</strong> Executor Main Loop</a></li><li class="chapter-item expanded "><a href="pre/single_final_code.html"><strong aria-hidden="true">3.5.</strong> Final Code</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">First layer - Future and IoWrapper</li><li class="chapter-item expanded "><a href="layer/fst/future_trait.html"><strong aria-hidden="true">4.</strong> Future trait</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer/fst/future_in_depth.html"><strong aria-hidden="true">4.1.</strong> Future in depth</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer/fst/mechanism.html"><strong aria-hidden="true">4.1.1.</strong> Future trait mechanism</a></li><li class="chapter-item expanded "><a href="layer/fst/fsm.html"><strong aria-hidden="true">4.1.2.</strong> Future internal - a Finite State Machine</a></li><li class="chapter-item expanded "><a href="layer/fst/challenge.html"><strong aria-hidden="true">4.1.3.</strong> The challenge of managing Future objects</a></li></ol></li><li class="chapter-item expanded "><a href="layer/fst/handling.html"><strong aria-hidden="true">4.2.</strong> Generic Future handling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer/fst/heap_alloc.html"><strong aria-hidden="true">4.2.1.</strong> General heap-allocated Future object</a></li><li class="chapter-item expanded "><a href="layer/fst/pool.html"><strong aria-hidden="true">4.2.2.</strong> Global Reusable Object Pool for allocation reuse, fragment control, and easy management</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="layer/fst/io_wrapper.html"><strong aria-hidden="true">5.</strong> IO Adapter for general file descriptor - IoWrapper</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer/fst/io_handling.html"><strong aria-hidden="true">5.1.</strong> General IO handling</a></li><li class="chapter-item expanded "><a href="layer/fst/io_wrapper_design.html"><strong aria-hidden="true">5.2.</strong> IoWrapper design</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">Second layer - Executor and message passing</li><li class="chapter-item expanded "><a href="layer/snd/executor.html"><strong aria-hidden="true">6.</strong> Executor</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer/snd/commands.html"><strong aria-hidden="true">6.1.</strong> General commands of a runtime</a></li><li class="chapter-item expanded "><a href="layer/snd/message_handling.html"><strong aria-hidden="true">6.2.</strong> Design of Executor</a></li></ol></li><li class="chapter-item expanded "><a href="layer/snd/message_passing.html"><strong aria-hidden="true">7.</strong> Passing messages</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer/snd/spawner.html"><strong aria-hidden="true">7.1.</strong> Spawner - a message sender</a></li><li class="chapter-item expanded "><a href="layer/snd/message_payload.html"><strong aria-hidden="true">7.2.</strong> Message payload</a></li></ol></li><li class="chapter-item expanded "><a href="layer/snd/join_handle.html"><strong aria-hidden="true">8.</strong> Join Handle for Future</a></li><li class="chapter-item expanded affix "><li class="part-title">Third layer - Scheduler and schedule problems</li><li class="chapter-item expanded "><a href="layer/trd/scheduler.html"><strong aria-hidden="true">9.</strong> Scheduler</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer/trd/scheduler_trait.html"><strong aria-hidden="true">9.1.</strong> Trait design</a></li><li class="chapter-item expanded "><a href="layer/trd/worker_structure.html"><strong aria-hidden="true">9.2.</strong> General Worker structure and logic</a></li><li class="chapter-item expanded "><a href="layer/trd/schedule_procedure.html"><strong aria-hidden="true">9.3.</strong> The procedure of task scheduling</a></li></ol></li><li class="chapter-item expanded "><a href="layer/trd/scheduling_method.html"><strong aria-hidden="true">10.</strong> Scheduling Method</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer/trd/round_robin.html"><strong aria-hidden="true">10.1.</strong> Round Robin</a></li><li class="chapter-item expanded "><a href="layer/trd/work_stealing.html"><strong aria-hidden="true">10.2.</strong> Work Stealing</a></li><li class="chapter-item expanded "><a href="layer/trd/hybrid.html"><strong aria-hidden="true">10.3.</strong> Hybrid Queue for Prioritized Work Stealing</a></li></ol></li><li class="chapter-item expanded "><a href="layer/trd/token_bucket.html"><strong aria-hidden="true">11.</strong> A token bucket like algorithm for auto task yielding</a></li><li class="chapter-item expanded affix "><li class="part-title">Fourth layer - Reactor and Waker handling</li><li class="chapter-item expanded "><a href="layer/fth/reactor.html"><strong aria-hidden="true">12.</strong> System IO Event Harvester - Reactor</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer/fth/io_registration.html"><strong aria-hidden="true">12.1.</strong> IO event registration</a></li><li class="chapter-item expanded "><a href="layer/fth/loop.html"><strong aria-hidden="true">12.2.</strong> Poll loop</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">Unresolved Problems and Future Works</li><li class="chapter-item expanded "><a href="prob/load_balancing.html"><strong aria-hidden="true">13.</strong> Load Balancing</a></li><li class="chapter-item expanded "><a href="prob/reactor_abstract.html"><strong aria-hidden="true">14.</strong> Reactor abstraction for different systems</a></li><li class="chapter-item expanded affix "><li class="part-title">References</li><li class="chapter-item expanded "><a href="references.html"><strong aria-hidden="true">15.</strong> References</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The Design and Implementation Detail of my-async</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>This book is about the design and the implementation of <code>my-async</code>, a Rust asynchronous IO runtime.
Why create a new runtime instead of using <code>tokio</code>? The problem with <code>tokio</code> is that its codebase is
vast and hard to track down, a least for me.</p>
<p>When I was trying to understand <code>tokio</code>'s approach to asynchronous runtime,
there were more than 35000 lines of code; it's just too much for a CS student like me.
So I decided to work on a new runtime with clear documentation
regarding the design and implementation as my graduate project.</p>
<p><code>my-async</code> has the following goals:</p>
<ul>
<li>A convenient interface to wrap over <code>AsFd</code> types.</li>
<li>A <code>Scheduler</code> trait that makes applying a new scheduling strategy easy.</li>
<li>A relatively short code that is easy to read.</li>
<li>Clear documentation that can express its design and implementation.</li>
</ul>
<p>The code repo is here: <a href="https://github.com/smb374/my-async-rs">https://github.com/smb374/my-async-rs</a>.</p>
<h2 id="note"><a class="header" href="#note">Note</a></h2>
<p>This project still needs some polishes as the code is implemented
by one person. If you spot any problems, open an issue
at the repo. I will look at it and try to fix it if there is spare time.</p>
<p>There may be many places in this book that may seem awkward or
based on my misunderstanding to certain concepts, please don't hesitate to point them out.</p>
<p>Also, as a non-native English user, there are multiple grammar &amp; spelling errors when writing this.
I will fix them as much as possible when I spot some,
but expect these errors when reading the book.</p>
<h2 id="special-thanks"><a class="header" href="#special-thanks">Special thanks</a></h2>
<p>I want to thank my supervisor for this project.
He helped me a lot with various concepts and spotted potential implementation flaws
during the development of this project.
Without him, the project may take longer to complete as I'm the only one
that works on the code of this project.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="async-in-rust"><a class="header" href="#async-in-rust">Async in Rust</a></h1>
<p>Async in Rust is simple and complex at the same time. If you're the user, you can do
the following stuff as an async 101:</p>
<pre><code class="language-rust">#[tokio::main]
async fn main() -&gt; io::Result&lt;()&gt; {
    println!(&quot;Hello async!&quot;);
    let result = async_io_fn_one().await?;
    println(&quot;Get IO result: {}&quot;, result);
    Ok(())
}
</code></pre>
<p>Just that simple.</p>
<p>On the other hand, if you want to dig inside, things can get quite tricky.</p>
<p>Async in Rust differs from other languages: it doesn't have a standard runtime.
The core language team decides to:</p>
<ol>
<li>Maintain flexibility over implementations that is suitable to different scenarios</li>
<li>Keep the size of the standard library small.</li>
</ol>
<p>The result of the decision is the <code>Future</code> trait. Anything that implements <code>Future</code> can
use the <code>.await</code> keyword to execute it asynchronously. The compiler can also implement
<code>Future</code> for normal function by adding <code>async</code> before the <code>fn</code> keyword.</p>
<p>The approach, however, causes a problem: by the design of the <code>Future</code> trait, it is lazy.
<code>Future</code> won't progress to the underlying code until someone uses the <code>poll()</code> method
to poll it. In other words, <code>Future</code> needs to be polled by some mechanism to work.
There are multiple ways to poll a <code>Future</code> object, most commonly by an executor.</p>
<p>The executor must be capable of dispatching, managing, and executing <code>Future</code>. The implementation of an executor can look very different, depending on the context, resources, and platform.
Generally, an executor can be divided into a few parts, that these parts will later discuss in the following sections.</p>
<p>Various executor implementations exist, such as <code>tokio</code>, <code>async-std</code>, <code>smol</code>, etc.
The problem is that they are incompatible since the executor must store numerous states to help
execute the <code>Future</code> objects. The problem is still unsolved now and hopefully can be solved one day.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-of-an-executors-architecture"><a class="header" href="#overview-of-an-executors-architecture">Overview of an executor's architecture</a></h1>
<p>The diagram of the executor's architecture:
<img src="pre/../assets/Executor_Layer_Bright.png" alt="Executor Layer" /></p>
<p>By the above diagram, we can divide the executor into four layers:</p>
<h2 id="first-layer---future-and-iowrapper"><a class="header" href="#first-layer---future-and-iowrapper">First Layer - Future and IoWrapper</a></h2>
<p>The job of this layer is to provide user interaction with the executor in a convenient way.
A global spawner is provided to send commands to the executor to spawn an async function or to shut down the executor.
The spawned function will return a join handle to join and retrieve the return value of that function.</p>
<p><code>IoWrapper</code> is provided for convenient <code>AsFd</code> type wrapping. It also provides <code>ref_io</code> and <code>mut_io</code> for those
IO actions that are not predefined to run as async.</p>
<h2 id="second-layer---executor"><a class="header" href="#second-layer---executor">Second layer - Executor</a></h2>
<p>This layer is the surface of the executor. The main job of this layer is to handle the messages from the spawner and command
the underlying scheduler and reactor by the corresponding message.</p>
<p>The executor will also set up the global spawner, scheduler with all the worker threads, and the reactor thread
on init for the runtime to work. After the executor is initialized, the user must <code>block_on</code> a single main
async function to fire up the runtime.</p>
<h2 id="third-layer---scheduler"><a class="header" href="#third-layer---scheduler">Third layer - Scheduler</a></h2>
<p>This layer will adapt the defined scheduling strategy to schedule async tasks for the workers.
The provided <code>Scheduler</code> trait is an abstract layer that defines a scheduler's basic behavior for the executor.
This is crucial for the new scheduling strategy to plug into this runtime easily.</p>
<p>Currently, <code>RoundRobinScheduler</code>, <code>WorkStealingScheduler</code>, and <code>HybridScheduler</code> is implemented by default.</p>
<h2 id="fourth-layer---reactor-and-waker-handling"><a class="header" href="#fourth-layer---reactor-and-waker-handling">Fourth layer - Reactor and Waker Handling</a></h2>
<p>This layer will communicate with the system and harvest all IO events reported. Once the IO events are gathered,
the reactor will:</p>
<ol>
<li>If wakers are registered related to an event, wake it up.</li>
<li>If not, store it in a table for later usage.</li>
</ol>
<p>Since the underlying call is handled by <code>mio</code> that is edge-triggered, we need to check the stored events
with currently registered wakers to make sure every event is consumed.</p>
<p>The details of the layers will be discussed later in the following chapters, but first, I will give an implementation
of a single-threaded executor to help you understand the main idea of an executor, then we'll move on to the multi-threaded version.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="a-minimal-single-threaded-future-evaluator"><a class="header" href="#a-minimal-single-threaded-future-evaluator">A minimal single-threaded Future evaluator</a></h1>
<p>This section will give a simple single-threaded executor implementation
for you to understand the whole picture of the following chapters, as</p>
<ol>
<li>It doesn't require a scheduler or any synchronization.</li>
<li>The reactor is a relatively minor factor in expressing the idea. We'll talk about it later.</li>
</ol>
<p>If you want to see the reactor's part first, see <a href="pre/../layer/fth/reactor.html">System IO Event Harvester - Reactor</a>
and the following subsections.</p>
<p>Now, let's get started with <code>Future</code> handling.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="future-handling"><a class="header" href="#future-handling">Future handling</a></h1>
<p>Since the compiler will generate any <code>async fn</code> functions as <code>Future</code> objects,
the generic type will be a hurdle to program, allocating these <code>Future</code> objects
as boxed trait objects is more sensible and easier to program.</p>
<p>First we define a <code>BoxedLocal&lt;T&gt;</code> type alias and <code>BoxedFuture</code> type:</p>
<pre><code class="language-rust  noplaypen">type BoxedLocal&lt;T&gt; = Pin&lt;Box&lt;dyn Future&lt;Output = T&gt; + 'static&gt;&gt;;

struct BoxedFuture {
    future: RefCell&lt;Option&lt;BoxedLocal&lt;io::Result&lt;()&gt;&gt;&gt;&gt;,
}
</code></pre>
<p>Here we use <code>RefCell</code> for internal mutability, and we assumed that all
enclosed should return <code>std::io::Result&lt;()&gt;</code> as <code>Future</code> requires mutating itself
by its trait definition and most of the tasks are IO bounded to be used under asynchronous environments.</p>
<p>To handle the underlying <code>async fn</code>, we define the <code>run</code> function as the following:</p>
<pre><code class="language-rust">impl BoxedFuture {
    fn run(&amp;self, index: &amp;FutureIndex, tx: Sender&lt;FutureIndex&gt;) -&gt; bool {
        let mut guard = self.future.borrow_mut();
        // run *ONCE*
        if let Some(fut) = guard.as_mut() {
            let new_index = FutureIndex {
                key: index.key,
            };
            // Create a waker that sends back the future to the process queue
            // once it's woke by that reactor.
            let waker = waker_fn(move || {
                tx.send(new_index).expect(&quot;Too many message queued!&quot;);
            });
            // Create a Context from the waker we just created.
            let cx = &amp;mut Context::from_waker(&amp;waker);
            // Poll the underlying future with cx
            match fut.as_mut().poll(cx) {
                Poll::Ready(r) =&gt; {
                    if let Err(e) = r {
                        // log error to logging facility
                        log::error!(&quot;Error occurred when executing future: {}&quot;, e);
                    }
                    true // Return value ready
                }
                Poll::Pending =&gt; false, // Pending
            }
        } else {
            true // Finished already
        }
    }
}
</code></pre>
<p>The process goes by:</p>
<ol>
<li>Check if it's done already for handling spurious call.</li>
<li>Create a waker that sends the index of itself back to process queue.</li>
<li>Create a <code>Context</code> for future polling using the waker we've just created.</li>
<li>Poll the future and match the result.</li>
</ol>
<p>Next, we'll talk about the global storage.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="global-storage"><a class="header" href="#global-storage">Global Storage</a></h1>
<p>By the design from me, every future should store in a global allocation pool for
allocation reuse and easy management. The details will be discussed in the
<a href="pre/../layer/fst/pool.html">Global Reusable Object Pool for fragment control and Future management</a>
part.</p>
<p>We first define <code>FUTURE_POOL</code> and <code>SPAWNER</code> as thread local objects as Rust assumes
you are under multithreaded environment without specific instruction. This will
cause some bounds for global variables to guarantee memory safety.</p>
<pre><code class="language-rust">thread_local! {
    static SPAWNER: RefCell&lt;Option&lt;Spawner&gt;&gt; = RefCell::new(None);
    static FUTURE_POOL: Pool&lt;BoxedFuture&gt; = Pool::new();
}
</code></pre>
<p>Since <code>Pool</code> is already lock-free, we don't need to use <code>RefCell</code> to encapsulate it.
The <code>SPAWNER</code> is used as a message sender and will be discussed in the next section.</p>
<p>We also need to define a <code>FutureIndex</code> that contains the key returned by the <code>Pool</code> and other
payloads (Though there are no other payloads in this case):</p>
<pre><code class="language-rust">#[derive(Clone, Copy, Eq)]
struct FutureIndex {
    key: usize,
}

impl PartialEq for FutureIndex {
    fn eq(&amp;self, other: &amp;Self) -&gt; bool {
        self.key == other.key
    }
}

impl Hash for FutureIndex {
    fn hash&lt;H: std::hash::Hasher&gt;(&amp;self, state: &amp;mut H) {
        self.key.hash(state);
    }
}
</code></pre>
<p>and implement <code>Clear</code> for the <code>BoxedFuture</code> for the allocation reserve part:</p>
<pre><code class="language-rust">impl Clear for BoxedFuture {
    fn clear(&amp;mut self) {
        self.future.borrow_mut().clear();
    }
}
</code></pre>
<p>Next, we'll move on to the message passing part.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="message-passing"><a class="header" href="#message-passing">Message Passing</a></h1>
<p>The <code>SPAWNER</code> we defined in the last part is used as a global message sender,
the underlying type is defined as:</p>
<pre><code class="language-rust">struct Spawner {
    tx: Sender&lt;Message&gt;,
}

enum Message {
    Run(FutureIndex),
    Close,
}
</code></pre>
<p>while the receiver half is held by the executor.
The are two messages: <code>Run</code> and <code>Close</code>:</p>
<ul>
<li><code>Run</code> is used to put the spawned future into the process queue.</li>
<li><code>Close</code> is used to signal the executor to shutdown.</li>
</ul>
<p>With the messages defined, we need to implement the corresponding functions:</p>
<pre><code class="language-rust">impl Spawner {
    fn spawn&lt;F&gt;(&amp;self, future: F)
    where
        F: Future&lt;Output = io::Result&lt;()&gt;&gt; + 'static,
    {
        // Alloc future inside the pool and retrieve its key to the entry.
        let key = FUTURE_POOL.with(|p| {
            p.create_with(|seat| {
                seat.future.borrow_mut().replace(Box::pin(future));
            })
            .unwrap()
        });
        // Send run command to the executor.
        self.tx
            .send(Message::Run(FutureIndex {
                key,
            }))
            .expect(&quot;too many task queued&quot;);
    }
    fn shutdown(&amp;self) {
        // Send shutdown command to the executor.
        self.tx.send(Message::Close).expect(&quot;too many task queued&quot;);
    }
}

// Corresponding public function for global spawner access.

pub fn spawn&lt;F&gt;(fut: F)
where
    F: Future&lt;Output = io::Result&lt;()&gt;&gt; + 'static,
{
    SPAWNER.with(|s| {
        if let Some(spawner) = s.borrow().as_ref() {
            spawner.spawn(fut);
        }
    })
}

pub fn shutdown() {
    SPAWNER.with(|s| {
        if let Some(spawner) = s.borrow().as_ref() {
            spawner.shutdown();
        }
    })
}
</code></pre>
<p>Note that the join handle is not implemented in this single-threaded executor for simplicity.
For the join handle implementation, please look <a href="pre/../layer/snd/join_handle.html">Join Handle for Future</a>.</p>
<p>Finally, we come to the main loop that handles the message.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="executor-main-loop"><a class="header" href="#executor-main-loop">Executor Main Loop</a></h1>
<p>Inside the main loop, there are a few stages:</p>
<ol>
<li>Pop and make a certain of progress of all futures in the waiting queue.</li>
<li>Try to receive to receive as much as futures that is waken up by the reactor.</li>
<li>Try to receive any message from the spawner.</li>
<li>Finally, wait for reactor to harvest events.</li>
</ol>
<h2 id="stage-1"><a class="header" href="#stage-1">Stage 1</a></h2>
<pre><code class="language-rust">let mut reactor = reactor::Reactor::default();
reactor.setup_registry();
'outer: loop {
    if let Some(index) = self.queue.pop_back() {
        FUTURE_POOL.with(|p| {
            if let Some(boxed) = p.get(index.key) {
                let finished = boxed.run(&amp;index, self.task_tx.clone());
                if finished &amp;&amp; !p.clear(index.key) {
                    log::error!(
                        &quot;Failed to remove completed future with index = {} from pool.&quot;,
                        index.key
                    );
                }
            } else {
                log::error!(&quot;Future with index = {} is not in pool.&quot;, index.key);
            }
        });
    } else {
      // Other stages
    }
}
</code></pre>
<p>Here we first setup the reactor for later use, and start popping <code>FutureIndex</code>s from
waiting queue. The error handling here is simply log the errors to the logging facility
for maintaining a short code. The process can be addressed as:</p>
<ol>
<li>Retrieve the <code>BoxedFuture</code> by the key of <code>FutureIndex</code>.</li>
<li>Use <code>BoxedFuture::run()</code> to make progress with the return value indicating id it's finished.</li>
<li>If it's finished, delete it from the global storage.</li>
</ol>
<h2 id="stage-2"><a class="header" href="#stage-2">Stage 2</a></h2>
<pre><code class="language-rust">'outer: loop {
    if let Some(index) = self.queue.pop_back() {
        // Stage 1
    } else {
        let mut wakeup_count = 0;
        loop {
            match self.task_rx.try_recv() {
                Ok(index) =&gt; {
                    wakeup_count += 1;
                    self.queue.push_front(index);
                }
                Err(TryRecvError::Empty) =&gt; break,
                Err(TryRecvError::Disconnected) =&gt; break 'outer,
            }
        }
        if wakeup_count &gt; 0 {
            continue;
        }
        // Other stages
    }
}
</code></pre>
<p>Here we use a counter to record the number of woke up futures. If there is any,
process those future first. The receive is non-blocking, so two possible error
will need to be handle:</p>
<ol>
<li><code>TryRecvError::Empty</code>: The channel is empty, stop trying to receive.</li>
<li><code>TryRecvError::Disconnected</code>: All the producer are disconnected, which means all futures are done, exit main loop directly.</li>
</ol>
<h2 id="stage-3-and-stage-4"><a class="header" href="#stage-3-and-stage-4">Stage 3 and Stage 4</a></h2>
<pre><code class="language-rust">'outer: loop {
    if let Some(index) = self.queue.pop_back() {
        // Stage 1
    } else {
        // Stage 2
        match self.rx.try_recv() {
            Ok(Message::Run(index)) =&gt; {
                self.queue.push_front(index);
            }
            Err(TryRecvError::Empty) =&gt; {
                if let Err(e) = reactor.wait(Some(Duration::from_millis(50))) {
                    log::error!(&quot;reactor wait error: {}, exit&quot;, e);
                    break;
                }
            }
            Ok(Message::Close) | Err(TryRecvError::Disconnected) =&gt; break,
        }
    }
}
</code></pre>
<p>These are the last stage in the main loop. Here the runtime will wait for any schedule messages
send by the global spawner, whether to run a new future or shutdown the runtime.
If there are no messages from the global spawner, we start to wait for reactor to wake up some futures.</p>
<p>Note that we need to check messages from global spawner at the same time, so we can't use indefinite
waiting on reactor. A time period need to be chosen to block the loop for a decent small amount of time
without spuriously wake up the main thread too frequently.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="final-code"><a class="header" href="#final-code">Final Code</a></h1>
<p>The final code is shown as below:</p>
<pre><code class="language-rust">use super::reactor;

use std::{
    cell::RefCell,
    collections::VecDeque,
    future::Future,
    hash::Hash,
    io,
    pin::Pin,
    rc::Rc,
    task::{Context, Poll},
    time::Duration,
};

use flume::{Receiver, Sender, TryRecvError};
use sharded_slab::{Clear, Pool};
use waker_fn::waker_fn;

thread_local! {
    static SPAWNER: RefCell&lt;Option&lt;Spawner&gt;&gt; = RefCell::new(None);
    static FUTURE_POOL: Pool&lt;BoxedFuture&gt; = Pool::new();
}

type BoxedLocal&lt;T&gt; = Pin&lt;Box&lt;dyn Future&lt;Output = T&gt; + 'static&gt;&gt;;

#[derive(Clone, Copy, Eq)]
struct FutureIndex {
    key: usize,
}

impl PartialEq for FutureIndex {
    fn eq(&amp;self, other: &amp;Self) -&gt; bool {
        self.key == other.key
    }
}

impl Hash for FutureIndex {
    fn hash&lt;H: std::hash::Hasher&gt;(&amp;self, state: &amp;mut H) {
        self.key.hash(state);
    }
}

struct BoxedFuture {
    future: RefCell&lt;Option&lt;BoxedLocal&lt;io::Result&lt;()&gt;&gt;&gt;&gt;,
}

impl Default for BoxedFuture {
    fn default() -&gt; Self {
        BoxedFuture {
            future: RefCell::new(None),
        }
    }
}

impl Clear for BoxedFuture {
    fn clear(&amp;mut self) {
        self.future.borrow_mut().clear();
    }
}

impl BoxedFuture {
    fn run(&amp;self, index: &amp;FutureIndex, tx: Sender&lt;FutureIndex&gt;) -&gt; bool {
        let mut guard = self.future.borrow_mut();
        // run *ONCE*
        if let Some(fut) = guard.as_mut() {
            let new_index = FutureIndex {
                key: index.key,
            };
            let waker = waker_fn(move || {
                tx.send(new_index).expect(&quot;Too many message queued!&quot;);
            });
            let cx = &amp;mut Context::from_waker(&amp;waker);
            match fut.as_mut().poll(cx) {
                Poll::Ready(r) =&gt; {
                    if let Err(e) = r {
                        log::error!(&quot;Error occurred when executing future: {}&quot;, e);
                    }
                    true
                }
                Poll::Pending =&gt; false,
            }
        } else {
            true
        }
    }
}

enum Message {
    Run(FutureIndex),
    Close,
}

pub struct Executor {
    task_tx: Sender&lt;FutureIndex&gt;,
    task_rx: Receiver&lt;FutureIndex&gt;,
    queue: VecDeque&lt;FutureIndex&gt;,
    rx: Receiver&lt;Message&gt;,
}

struct Spawner {
    tx: Sender&lt;Message&gt;,
}

impl Executor {
    pub fn new() -&gt; Self {
        let (tx, rx) = flume::unbounded();
        let (task_tx, task_rx) = flume::unbounded();
        let spawner = Spawner { tx };
        SPAWNER.with(|s| s.borrow_mut().replace(spawner));
        Self {
            task_tx,
            task_rx,
            queue: VecDeque::with_capacity(1024),
            rx,
        }
    }

    fn run(&amp;mut self) {
        let mut reactor = reactor::Reactor::default();
        reactor.setup_registry();
        'outer: loop {
            if let Some(index) = self.queue.pop_back() {
                FUTURE_POOL.with(|p| {
                    if let Some(boxed) = p.get(index.key) {
                        let finished = boxed.run(&amp;index, self.task_tx.clone());
                        if finished &amp;&amp; !p.clear(index.key) {
                            log::error!(
                                &quot;Failed to remove completed future with index = {} from pool.&quot;,
                                index.key
                            );
                        }
                    } else {
                        log::error!(&quot;Future with index = {} is not in pool.&quot;, index.key);
                    }
                });
            } else {
                let mut wakeup_count = 0;
                loop {
                    match self.task_rx.try_recv() {
                        Ok(index) =&gt; {
                            wakeup_count += 1;
                            self.queue.push_front(index);
                        }
                        Err(TryRecvError::Empty) =&gt; break,
                        Err(TryRecvError::Disconnected) =&gt; break 'outer,
                    }
                }
                if wakeup_count &gt; 0 {
                    continue;
                }
                match self.rx.try_recv() {
                    Ok(Message::Run(index)) =&gt; {
                        self.queue.push_front(index);
                    }
                    Err(TryRecvError::Empty) =&gt; {
                        if let Err(e) = reactor.wait(Some(Duration::from_millis(50))) {
                            log::error!(&quot;reactor wait error: {}, exit&quot;, e);
                            break;
                        }
                    }
                    Ok(Message::Close) | Err(TryRecvError::Disconnected) =&gt; break,
                }
            }
        }
    }
    pub fn block_on&lt;F&gt;(mut self, future: F) -&gt; F::Output
    where
        F: Future + 'static,
    {
        let result_arc: Rc&lt;RefCell&lt;Option&lt;F::Output&gt;&gt;&gt; = Rc::new(RefCell::new(None));
        let clone = Rc::clone(&amp;result_arc);
        spawn(async move {
            let result = future.await;
            clone.borrow_mut().replace(result);
            log::debug!(&quot;Blocked future finished.&quot;);
            shutdown();
            Ok(())
        });
        log::info!(&quot;Start blocking...&quot;);
        self.run();
        log::debug!(&quot;Waiting result...&quot;);
        let mut guard = result_arc.borrow_mut();
        let result = guard.take();
        assert!(
            result.is_some(),
            &quot;The blocked future should produce a return value before the execution ends.&quot;
        );
        result.unwrap()
    }
}

impl Drop for Executor {
    fn drop(&amp;mut self) {
        SPAWNER.with(|s| {
            if let Some(spawner) = s.borrow().as_ref() {
                spawner
                    .tx
                    .send(Message::Close)
                    .expect(&quot;Message queue is full.&quot;);
            }
        });
    }
}

impl Default for Executor {
    fn default() -&gt; Self {
        Self::new()
    }
}

impl Spawner {
    fn spawn&lt;F&gt;(&amp;self, future: F)
    where
        F: Future&lt;Output = io::Result&lt;()&gt;&gt; + 'static,
    {
        let key = FUTURE_POOL.with(|p| {
            p.create_with(|seat| {
                seat.future.borrow_mut().replace(Box::pin(future));
            })
            .unwrap()
        });
        self.tx
            .send(Message::Run(FutureIndex { key }))
            .expect(&quot;too many task queued&quot;);
    }
    fn shutdown(&amp;self) {
        self.tx.send(Message::Close).expect(&quot;too many task queued&quot;);
    }
}

pub fn spawn&lt;F&gt;(fut: F)
where
    F: Future&lt;Output = io::Result&lt;()&gt;&gt; + 'static,
{
    SPAWNER.with(|s| {
        if let Some(spawner) = s.borrow().as_ref() {
            spawner.spawn(fut);
        }
    })
}

pub fn shutdown() {
    SPAWNER.with(|s| {
        if let Some(spawner) = s.borrow().as_ref() {
            spawner.shutdown();
        }
    })
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="future-trait"><a class="header" href="#future-trait">Future trait</a></h1>
<p><code>Future</code> trait is the core of asynchronous IO in Rust.
Without it, the <code>async</code>/<code>await</code> keyword won't work, and
designer of a runtime will have to implement a callback
system to cooperate with the system IO event handle
loop.</p>
<p>In this section, we will take a deeper look into
the <code>Future</code> trait and talk about the technique I
use to handle it.</p>
<p>Without a second thought, let's dig in...</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="future-in-depth"><a class="header" href="#future-in-depth">Future in depth</a></h1>
<p>In this part, we will:</p>
<ul>
<li>Desugar the generated <code>Future</code> object from the <code>async fn</code> functions by the compiler.</li>
<li>Take a deeper look into <code>Future</code>'s trait definition and the <code>Waker</code> mechanism.</li>
<li>Discuss the challenge we will face under this mechanism.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="future-trait-mechanism"><a class="header" href="#future-trait-mechanism">Future trait mechanism</a></h1>
<p>The <code>Future</code> trait is defined as:</p>
<pre><code class="language-rust">pub trait Future {
    type Output;

    fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt;;
}
</code></pre>
<p>The trait defines:</p>
<ol>
<li><code>Output</code>: The execution result of the underlying <code>Future</code>.</li>
<li><code>poll</code>: The method that triggers the execution of a <code>Future</code> object. Returns <code>Poll</code> with two states:
<ul>
<li><code>Poll::Ready(Output)</code>: The <code>Future</code> successfully executes with its return value.</li>
<li><code>Poll::Pending</code>: The <code>Future</code> is currently pending, check it later to get the execution result.</li>
</ul>
</li>
</ol>
<p>By design, a <code>Future</code> object is lazy unless someone triggers the <code>poll</code> method defined
by the trait. The <code>poll</code> method contains a few unusual structs in it, including:</p>
<ol>
<li><code>Pin</code></li>
<li><code>Context</code></li>
</ol>
<p>We will look into these two structs in the following sections.</p>
<h2 id="1-the-pinp-smart-pointer"><a class="header" href="#1-the-pinp-smart-pointer">1. The <code>Pin&lt;P&gt;</code> smart pointer</a></h2>
<p>The <code>Pin&lt;P&gt;</code> smart pointer is a wrapper around the <code>P</code> pointer that ensures the pointee
will not move out of its current address, i.e. the address that <code>P</code> points to. This effect is
canceled if the pointee type implements the <code>Unpin</code> auto trait.</p>
<p>For example, the <code>std::mem::swap</code> can swap the pointee of two pointers, which essentially &quot;moves&quot;
the pointee out of its original place. If we wrap the two pointers with <code>Pin</code>, the use of <code>swap</code>
is forbidden, the code will fail to compile.</p>
<p>The reason of the use of <code>Pin</code> is mostly for the self referral structures.
Consider the following structure:</p>
<pre><code class="language-rust">struct SelfRef {
    buf: [u8; 4096],
    p: &amp;[u8],
}
</code></pre>
<p>If the field <code>p</code> points to the field <code>buf</code>, when the whole struct is moved, the underlying fields will also be moved
while the pointer <code>p</code> is still pointing the location of <code>buf</code> before the move occurs. We can clearly see that
the following access <code>p</code> will cause undefined behavior since the pointee may be a different type or <code>p</code> becomes
a dangling pointer. With the use of <code>Pin</code>, we can avoid this kind of stuff and prevent undefined behavior.
These kinds of type will also implement <code>!Unpin</code> to make <code>Pin</code> has its effect on the type.</p>
<p>The reason that <code>poll</code> requires this bound is related to the next section. For now, we will discuss <code>Unpin</code> and <code>Context</code> first.</p>
<h3 id="the-unpin-auto-trait"><a class="header" href="#the-unpin-auto-trait">The <code>Unpin</code> auto trait</a></h3>
<p>The type with <code>Unpin</code> auto trait implemented will cancel the effect of <code>Pin</code>, e.g. <code>Pin&lt;Box&lt;T&gt;&gt;</code> is as same as <code>Box&lt;T&gt;</code> when <code>T: Unpin</code>.
The reason is that these types doesn't depend on its location to work properly. The only exception is those types who implements <code>!Unpin</code>.</p>
<p>There are only 4 kinds of types that implement <code>!Unpin</code>:</p>
<ol>
<li><code>Future</code> generated by the compiler with the <code>async fn</code> syntax.</li>
<li><code>PhantomPinned</code> marker type.</li>
<li>Structs that contain <code>!Unpin</code> type in its field.</li>
<li><code>unsafe impl !Unpin for [what ever type here]</code></li>
</ol>
<p>The reason of why the compiler generated <code>Future</code> will be discussed in the next section. Now let's focus on <code>Context</code> first.</p>
<h2 id="2-contexta"><a class="header" href="#2-contexta">2. <code>Context&lt;'a&gt;</code></a></h2>
<p>This type defines the context of an async task. It's currently used to provide the access to a reference of a <code>Waker</code> that can wake
up the current task.</p>
<p>A <code>Waker</code> is essentially a wrapper to a VTable (<code>RawWakerVTable</code>) and some extra payloads (<code>data</code> pointer). The VTable contains:</p>
<ol>
<li><code>clone</code>: Function to run when the waker is cloned.</li>
<li><code>wake</code>: Function to run when <code>Waker::wake</code> is called. It will consume the <code>data</code> pointer.</li>
<li><code>wake_by_ref</code>: Function to run when <code>Waker::wake_by_ref</code> is called. It won't consume the <code>data</code> pointer.</li>
<li><code>drop</code>: Function to run when the <code>Waker</code> is dropped.</li>
</ol>
<p>Typically, the creation of a <code>Waker</code> is done by some other libraries (e.g. <code>waker_fn</code> for my implementation).</p>
<!-- The `Waker`s' management will be discussed in [Waker handling](../fth/waker_handling.md) -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="future-internal---a-finite-state-machine"><a class="header" href="#future-internal---a-finite-state-machine">Future internal - a Finite State Machine</a></h1>
<p>In Rust, we can create an asynchronous function as simple as the following code:</p>
<pre><code class="language-rust">async fn async_func() {
    init_step();
    f().await;
    g().await;
    h().await;
    final_step();
}
</code></pre>
<p>From a user's perspective, we can quickly conclude that the function will run
<code>f()</code>, <code>g()</code>, <code>h()</code> in order asynchronously. But how this snippet is related
to the <code>Future</code> trait? It turns out that the compiler will translate the code snippet
into the following:</p>
<pre><code class="language-rust">struct AsyncFunc {
    fut_f: FutF,
    fut_g: FutG,
    fut_h: FutH,
    state: AsyncFuncState,
}

enum AsyncFuncState {
    Init,
    AwaitF,
    AwaitG,
    AwaitH,
    Final,
}

impl Future for AsyncFunc {
    type Output = ();

    fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt; {
        let me = self.get_mut();
        loop {
            match me.state {
                AsyncFuncState::Init =&gt; {
                    init_step();
                    me.state = AsyncFuncState::AwaitF;
                },
                AsyncFuncState::AwaitF =&gt; match me.fut_f.poll(cx) {
                    Poll::Ready(()) =&gt; me.state = AsyncFuncState::AwaitG,
                    Poll::Pending =&gt; break Poll::Pending,
                },
                AsyncFuncState::AwaitG =&gt; match me.fut_g.poll(cx) {
                    Poll::Ready(()) =&gt; me.state = AsyncFuncState::AwaitH,
                    Poll::Pending =&gt; break Poll::Pending,
                },
                AsyncFuncState::AwaitH =&gt; match me.fut_h.poll(cx) {
                    Poll::Ready(()) =&gt; me.state = AsyncFuncState::Final,
                    Poll::Pending =&gt; break Poll::Pending,
                },
                AsyncFuncState::Final =&gt; {
                    final_step();
                    break Poll::Ready(());
                },
            }
        }
    }
}
</code></pre>
<p>This code can be viewed as a finite state machine, and can be visualized as the following diagram:
<img src="https://i2.lensdump.com/i/RkBbkv.png" alt="" /></p>
<p>Since the code is compiled to a finite state machine, the state can store the arguments for the future to run,
the return value of previous future execution, etc. Because of this, the use of <code>Pin</code> and <code>!Unpin</code> is necessary.</p>
<h2 id="why-use-pin-and-why-the-generated-code-implements-unpin"><a class="header" href="#why-use-pin-and-why-the-generated-code-implements-unpin">Why use <code>Pin</code> and why the generated code implements <code>!Unpin</code>?</a></h2>
<p>Consider the following <code>async</code> block:</p>
<pre><code class="language-rust">async {
    let mut x = [0u8; 4096];
    let fut = read_to_buf(&amp;mut x);
    let n = fut.await;
    println!(&quot;got {:?}&quot;, &amp;x[..n]);
}
</code></pre>
<p>The compiler will generate the following structure:</p>
<pre><code class="language-rust">struct ReadToBuf&lt;'a&gt; {
    buf: &amp;'a mut [u8],
}
struct AsyncBlock1 {
    x: [u8; 4096],
    fut: ReadToBuf&lt;'_&gt;, // self refer to x field
    state: State,
}
</code></pre>
<p>We can see that the generated <code>AsyncBlock1</code> contains a future <code>ReadToBuf&lt;'_&gt;</code> that uses <code>&amp;mut x</code> for reading.
If this <code>async</code> block is moved, so will <code>x</code>, and since <code>x</code> is moved, the pointer in <code>ReadToBuf&lt;'_&gt;</code> can point
to an invalid location or dangle, causing undefined behavior. To prevent this, the generated <code>Future</code> object
will always be <code>!Unpin</code> to enable the effect that <code>Pin</code> brings, and thus the <code>poll</code> function takes
<code>self: Pin&lt;&amp;mut Self&gt;</code> rather than <code>&amp;mut self</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-challenge-of-managing-future-objects"><a class="header" href="#the-challenge-of-managing-future-objects">The challenge of managing Future objects</a></h1>
<p>With the previous sections, we can know that:</p>
<ol>
<li><code>Future</code> object is lazy.</li>
<li><code>Future</code> object needs a <code>Context</code>, which is obtained by a <code>Waker</code>, to be polled.</li>
<li>Generated <code>Future</code> object can contain numbers of anonymous states and fields.</li>
<li>Generated <code>Future</code> object is always <code>!Unpin</code>.</li>
</ol>
<p>Managing the generated <code>Future</code> objects is quite a hurdle, as it's impossible to type
any <code>Future</code> statically so that all the generated <code>Future</code> objects runs on stack,
and it's quite inefficient to actively poll every <code>Future</code> object you get with an
empty <code>Waker</code> (basically busy waiting).</p>
<p>In the next section, I will discuss how to manage these generated bastards using heap.
While it's not zero-cost, managing the generated <code>Futures</code> as heap-allocated trait objects
is much simpler than fighting with the borrow checker and the moving problems caused by <code>!Unpin</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="generic-future-handling"><a class="header" href="#generic-future-handling">Generic Future handling</a></h1>
<p>In this part, we will:</p>
<ul>
<li>Discuss how to handle generic <code>Future</code> objects.</li>
<li>Talk about my approach to solve this problem.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="general-heap-allocated-future-object"><a class="header" href="#general-heap-allocated-future-object">General heap-allocated Future object</a></h1>
<p>Rust has the following ways to handle generic objects:</p>
<ol>
<li>By <code>&lt;T: Trait + 'lifetime&gt;</code>: Specifies a type parameter with trait bound and lifetime bound.</li>
<li>By <code>impl Trait</code>: Use an anonymous type parameter bound by trait.</li>
<li>By <code>dyn Trait</code>: Use a trait object to dynamic dispatch type.</li>
</ol>
<p>Method 1 needs to know the exact type in compile time, and method 2 can be only used at function signature,
our only option is by method 3, trait objects.</p>
<p>Since a trait object is an opaque type that the size is only known at runtime,
we need to use a pointer to access it.
As storing on stack needs to play with lots of lifetime bounds, w only consider these heap-allocated pointer types:</p>
<ul>
<li><code>Pin&lt;Box&lt;dyn Future + AutoTraits + 'a&gt;&gt;</code></li>
<li><code>Pin&lt;Rc&lt;dyn Future + AutoTraits + 'a&gt;&gt;</code></li>
<li><code>Pin&lt;Arc&lt;dyn Future + AutoTraits + 'a&gt;&gt;</code></li>
</ul>
<p>A natural choice will be the <code>Box</code> one, since we only need to put it on heap.
You might think that doesn't <code>Future</code>'s <code>poll</code> function require <code>Pin&lt;&amp;mut Self&gt;</code>? We don't need internal mutability inside?
As it turns out, we can make use of <code>AsMut</code> trait, that using <code>.as_mut()</code> will result in:</p>
<pre><code>&amp;mut Pin&lt;P&lt;T&gt;&gt; -&gt; Pin&lt;&amp;mut T&gt;
</code></pre>
<p>We only need the internal mutability stuff outside:</p>
<pre><code class="language-rust">RefCell&lt;Pin&lt;Box&lt;dyn Future + 'a&gt;&gt;&gt; // Single thread.
Mutex&lt;Pin&lt;Box&lt;dyn Future + 'a&gt;&gt;&gt; // Multi thread.
</code></pre>
<p>These internal mutability needs a reference counted pointers to access properly, so we need to wrap it with <code>Rc</code> or <code>Arc</code>:</p>
<pre><code class="language-rust">Rc&lt;RefCell&lt;Pin&lt;Box&lt;dyn Future + 'a&gt;&gt;&gt;&gt; // Single thread.
Arc&lt;Mutex&lt;Pin&lt;Box&lt;dyn Future + 'a&gt;&gt;&gt;&gt; // Multi thread.
</code></pre>
<p>or:</p>
<pre><code class="language-rust">Pin&lt;Rc&lt;RefCell&lt;dyn Future + 'a&gt;&gt;&gt; // Single thread.
Pin&lt;Arc&lt;Mutex&lt;dyn Future + 'a&gt;&gt;&gt; // Multi thread.
</code></pre>
<p>At this point, you are good to go, although there may be a problem.
You see, you need to allocate every time a <code>Future</code> is spawned in your runtime. For a server type program
that spawns multiple handler that may be inefficient:</p>
<ol>
<li><code>Future</code> objects are allocated and destroyed multiple times, which can cause fragmentation depending on the global allocator.</li>
<li>The allocated space can actually be reused to reduce heap size and have a better performance as no allocation occurred when we reuse it.</li>
</ol>
<p>That's why I change to use a global object pool provided by <code>sharded_slab</code>.
The next section I will talk about how to use a global object pool to handle <code>Future</code> objects.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="global-reusable-object-pool-for-allocation-reuse-fragment-control-and-easy-management"><a class="header" href="#global-reusable-object-pool-for-allocation-reuse-fragment-control-and-easy-management">Global Reusable Object Pool for allocation reuse, fragment control, and easy management</a></h1>
<p><code>sharded_slab</code> provides <code>Pool</code> for object pool that can:</p>
<ol>
<li>Allocate inside the pool.</li>
<li>Retain the allocation inside the pool when an entry is cleared.</li>
<li>Reuse the retained allocation when allocation occurs.</li>
</ol>
<p>The pool requires <code>T: Clear + Default</code>, that the <code>Clear</code> trait is used to clear value and retain allocation.
We can use this to design the type we want to store. In this case, the following type is chosen:</p>
<pre><code class="language-rust">// type Boxed&lt;T&gt; = Pin&lt;Box&lt;dyn Future&lt;Output = T&gt; + Send + 'static&gt;&gt;;
pub(crate) struct BoxedFuture {
    pub(crate) future: Mutex&lt;Option&lt;Boxed&lt;io::Result&lt;()&gt;&gt;&gt;&gt;,
}
</code></pre>
<p>By default, <code>Option&lt;T&gt;</code> implements <code>Clear</code> by default, which uses <code>Option::take()</code> to do the clear job.
<code>Option::take()</code> itself is implemented by <code>std::mem::replace(self, None)</code>, which the <code>std::mem::replace</code>
won't dealloc the left-hand side. This property makes it capable to do the allocation retain job, so we can
put the <code>Boxed</code> type under it. The <code>Mutex</code> wraps around it is to provide internal mutability for <code>Future</code>'s <code>poll</code> requirement.</p>
<p>We can implement <code>Clear</code> for <code>BoxedFuture</code> with <code>Option::clear()</code>:</p>
<pre><code class="language-rust">impl Default for BoxedFuture {
    fn default() -&gt; Self {
        BoxedFuture {
            future: Mutex::new(None),
        }
    }
}

impl Clear for BoxedFuture {
    fn clear(&amp;mut self) {
        self.future.get_mut().clear();
    }
}
</code></pre>
<p>Then create a global future object pool with <code>Lazy</code> from <code>once_cell</code>:</p>
<pre><code class="language-rust">// global future allocation pool.
pub(crate) static FUTURE_POOL: Lazy&lt;Pool&lt;BoxedFuture&gt;&gt; = Lazy::new(Pool::new);
</code></pre>
<p>The key will be wrapped inside <code>FutureIndex</code> that contains other payloads:</p>
<pre><code class="language-rust">#[derive(Clone, Copy, Eq)]
pub struct FutureIndex {
    pub key: usize,
    // Other payloads...
}

impl PartialEq for FutureIndex {
    fn eq(&amp;self, other: &amp;Self) -&gt; bool {
        self.key == other.key
    }
}

impl Hash for FutureIndex {
    fn hash&lt;H: std::hash::Hasher&gt;(&amp;self, state: &amp;mut H) {
        self.key.hash(state);
    }
}
</code></pre>
<h2 id="the-overall-step-of-future-processing"><a class="header" href="#the-overall-step-of-future-processing">The overall step of <code>Future</code> processing</a></h2>
<ol>
<li>Allocate future object inside the pool when a future is spawned. The pool will return a key to access the entry where the future is.
<pre><code class="language-rust">// in Spawner::spawn_with_handle()
// ...
let key = FUTURE_POOL
    .create_with(|seat| {
        seat.future.get_mut().replace(spawn_fut.boxed());
    })
    .unwrap();
// ...
</code></pre>
</li>
<li>The key is used instead of a pointer to be processed by the message system and the schedule system.
<pre><code class="language-rust">// in Spawner::spawn_with_handle(), after step 1
// ...
self.tx
    .send(ScheduleMessage::Schedule(FutureIndex {
        key,
        // Other payloads...
    }))
    .expect(&quot;Failed to send message&quot;);
// ...
</code></pre>
</li>
<li>When the future is going to be processed:
<ol>
<li>The worker will use the key to gain the access of the future in pool.
<pre><code class="language-rust">// in scheduler::process_future()
if let Some(boxed) = FUTURE_POOL.get(index.key) {
    let finished = boxed.run(&amp;index, tx.clone());
} // ...
</code></pre>
</li>
<li>Lock the future and gain <code>&amp;mut</code> to use <code>Future::poll</code> and poll the future. Return if the future is <code>Ready</code>.
<pre><code class="language-rust">// in BoxedFuture::run()
let mut guard = self.future.lock();
// run *ONCE*
if let Some(fut) = guard.as_mut() {
    let waker = waker_fn(move || {
        // Create waker
    });
    let cx = &amp;mut Context::from_waker(&amp;waker);
    match fut.as_mut().poll(cx) {
        Poll::Ready(r) =&gt; {
            if let Err(e) = r {
                // log error
            }
            true
        }
        Poll::Pending =&gt; false,
    }
} else {
    true
}
</code></pre>
</li>
</ol>
</li>
<li>If the future is <code>Poll::Ready</code>, clear the future for future allocation reuse.
<pre><code class="language-rust">// in scheduler::process_future()
if finished {
    // ...
    if !FUTURE_POOL.clear(index.key) {
        // ...
    }
}
</code></pre>
</li>
</ol>
<h2 id="note-1"><a class="header" href="#note-1">Note</a></h2>
<p>The <code>Future</code> object is now single accessed by a single thread, why use <code>Mutex</code> instead of <code>RefCell</code> as they both implement <code>Send</code>?
This is because that the pool itself needs <code>Sync</code> for the global pool encapsulate in <code>Lazy</code> to work properly in multithreaded environment.
We need to use <code>Mutex</code> to make the type store inside the pool <code>Sync</code>.</p>
<p>The <code>Mutex</code> will have small impact since it's already single accessed, its function will be like a <code>RefCell</code> that the thread-blocking function is canceled by our design.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="io-adapter-for-general-file-descriptor---iowrapper"><a class="header" href="#io-adapter-for-general-file-descriptor---iowrapper">IO Adapter for general file descriptor - IoWrapper</a></h1>
<p>Other than <code>Future</code> object handling, we also need IO source to make
<code>async</code> a thing as IO is the reason why we need asynchronous runtime.</p>
<p>In this section, we will talk about:</p>
<ol>
<li>General IO handling</li>
<li><code>IoWrapper</code> design</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="general-io-handling"><a class="header" href="#general-io-handling">General IO handling</a></h1>
<p>Generally, when using asynchronous runtime in Rust, you're dealing with non-blocking IO,
you'll want to make the file descriptors you're working on.</p>
<p>Setting a file descriptor is easy by using <code>fcntl</code> to set the <code>O_NONBLOCK</code>.</p>
<p>Next we'll talk about the error handling part.</p>
<p>A non-blocking IO handling can be described by the following pseudocode:</p>
<pre><code>run_nbio(io):
    n &lt;- io()
    if n == -1 then
        if (errno == EWOULDBLOCK) then
            // The io action will block, try again later
            return PENDING, nil
        else if (errno == EINTERRUPT) then
            // The action is interrupted by system, retry immediately
            return nbio()
        else
            // io is completed with error
            // other errors are handled by the user
            return READY, -1
        end
    else
        // io is completed
        return READY, n
    end
</code></pre>
<p>Basically the code will only handle two kinds of error itself: <code>EWOULDBLOCK</code> and <code>EINTERRUPT</code>:</p>
<ol>
<li><code>EWOULDBLOCK</code> means that <code>io()</code> will block the thread, return <code>PENDING</code> to make the user try again later.</li>
<li><code>EINTERRUPT</code> means that an interrupt occurs when running <code>io()</code>, retry immediately.</li>
</ol>
<p>We can then bring this to Rust:</p>
<pre><code class="language-rust">// in IoWrapper::poll_ref()
fn poll_ref&lt;U, F&gt;(
    &amp;self,
    cx: &amp;mut Context&lt;'_&gt;, // context for polling
    interest: Interest, // READ or WRITE
    mut f: F, // action
) -&gt; Poll&lt;io::Result&lt;U&gt;&gt;
where
    F: FnMut(&amp;Self) -&gt; io::Result&lt;U&gt;,
{
    match f(self) {
        // WouldBlock, do it when the io is available.
        Err(ref e) if e.kind() == io::ErrorKind::WouldBlock =&gt; {
            let current = self.token.load(Ordering::Relaxed);
            // Register to the reactor.
            // The reactor will wake the task to run this again
            // when it gets the event correspoding to the interst.
            self.register_reactor(current, interest, cx)?;
            Poll::Pending
        }
        // Interrupt, retry immediately
        Err(ref e) if e.kind() == io::ErrorKind::Interrupted =&gt; self.poll_ref(cx, interest, f),
        // Ready with Success or other Error
        r =&gt; Poll::Ready(r),
    }
}
</code></pre>
<p>Next, we'll talk about <code>IoWrapper</code>'s design.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="iowrapper-design"><a class="header" href="#iowrapper-design">IoWrapper design</a></h1>
<p><code>IoWrapper</code> is a wrapper for IO sources like <code>File</code>, <code>TcpStream</code>, etc. to become an IO event provider
for the runtime. The wrapper provides various defaults and methods for users to define a new IO source quickly.</p>
<p>The <code>IoWrapper</code> is defined as:</p>
<pre><code class="language-rust">pub struct IoWrapper&lt;T: AsFd&gt; {
    inner: T,
    token: AtomicUsize,
}
</code></pre>
<p>Where <code>inner</code> is the type being wrapped, and <code>token</code> is the identifier.</p>
<p>The wrapped type is bounded by <code>AsFd</code> since the runtime only accepts registering IO events related
to system IO, the wrapper thus requires the types to be wrapped should be <code>AsFd</code> that can extract
the underlying file descriptor.</p>
<p><code>IoWrapper</code> provides:</p>
<ul>
<li>A convenient wrapper over <code>AsFd</code> types that set to non-blocking mode automatically.</li>
<li><code>ref_io</code> and <code>mut_io</code> for IO that doesn't require/require mutating self.
<ul>
<li><code>ref_io</code> and <code>mut_io</code> are also asynchronous function by themselves, you don't need to implement <code>Future</code> yourself.</li>
<li>You can use these function to perform IO operations that aren't defined by the runtime or other asynchronous traits.</li>
<li>For usage, see <a href="https://smb374.github.io/my-async-rs/api_references/my_async/struct.IoWrapper.html">API documentation</a></li>
</ul>
</li>
<li>Auto implementation of <code>AsyncRead</code> and <code>AsyncWrite</code> for types implementing <code>std::io::Read</code> and <code>std::io::Write</code>.</li>
<li>Get reference/mutable reference of the inner type by <code>IoWrapper::inner()</code>/<code>IoWrapper::inner_mut()</code>.</li>
</ul>
<p>In addition to these features, since a <code>IoWrapper</code> wraps over any <code>AsFd</code> type with a token, we can register it with the
token to the reactor of the runtime. The reactor will then look after its events, and thus can wake up tasks related to
it corresponding to <code>Interest::READABLE</code> or <code>Interest::WRITABLE</code>. In this way, <code>IoWrapper</code> can play the role of IO
events provider to wake asynchronous tasks. The rest of the details will be discussed in the
subsections of <a href="layer/fst/../fth/reactor.html">System IO Event Harvester - Reactor</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="executor"><a class="header" href="#executor">Executor</a></h1>
<p>In the previous section, we've described:</p>
<ol>
<li>How to properly handle compiler generated <code>Future</code> objects</li>
<li><code>IoWrapper</code> that is capable of being an IO source that provides IO events for waking tasks.</li>
</ol>
<p>But defining these won't fire up the execution procedure, we still need an executor to process these
tasks. In this section, we'll discuss:</p>
<ol>
<li><code>Executor</code> for executing <code>Future</code>.
<ul>
<li>General commands of a runtime.</li>
<li><code>Executor</code>'s design.</li>
</ul>
</li>
<li>Passing messages.
<ul>
<li><code>Spawner</code> - a message sender.</li>
<li>Message payload.</li>
</ul>
</li>
<li><code>JoinHandle</code> for spawned future.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="general-commands-of-a-runtime"><a class="header" href="#general-commands-of-a-runtime">General commands of a runtime</a></h1>
<p>Imagine you're a user that holds a pool of <code>Future</code> objects that is ready to execute, and the <code>Executor</code>
as a server that is capable to execute your <code>Future</code> objects with their results return to you.</p>
<p>Naturally, you'll want to send a request that asks the server to execute your task. You can, of course, choose
to wait until the server finishes the task and retrieves its result, or you can choose to tell the server: Give
me a handle, and I'll check its status later. Let's call the latter one <code>spawn</code>.</p>
<p>We can show the differences with the following snippet:</p>
<pre><code class="language-rust">// Wait until `f()` finishes and retrieve its result.
async fn af1() {
    let result = f().await;
    // Do the rest of stuff...
}
async fn af2() {
    let handle = spawn(f());
    // Do other stuff...
    // Check the result of execution.
    let result = handle.join().await;
}
</code></pre>
<p>As you can see, we need to define a <code>spawn</code> function to handle these kinds of requests.
Note that in the first case, no extra <code>Future</code> object is created as it will become a state of the <code>Future</code>
object generated by <code>af1()</code>'s code, so we don't need a command to handle it.</p>
<p>Now, let's say that you're done with the works, and you want to shut down the server. Naturally,
you'll call the <code>shutdown</code> command to close the server before exit.</p>
<p>By the above situation, we can see that we need two essential commands: <code>spawn()</code> and <code>shutdown()</code> to control
the <code>Executor</code> we need to use. Of course there can be more commands, but for <code>my-async</code>, these commands are enough.
Now, let's talk about how we can design the <code>Executor</code> we need with the requirements.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="design-of-executor"><a class="header" href="#design-of-executor">Design of Executor</a></h1>
<p>By the previous section, we can now create a pseudocode to illustrate the command handling:</p>
<pre><code>HandleRequests():
    while true:
        r &lt;- GetRequest()
        match r:
            Spawn(Task) -&gt; Put the Task to the execution system
            Shutdown -&gt; break
</code></pre>
<p>The execution system is done by the <code>Scheduler</code>, and we also need to call <code>Reactor</code> to look up for
IO events to wake tasks. By these facts and the above pseudocode, the <code>Executor</code> can be viewed
as an abstraction layer that handles messages and relay the actual actions to <code>Scheduler</code> and <code>Reactor</code>.</p>
<p>In the code, the <code>Executor</code> will do almost the same thing with one more message to handle. The <code>Executor</code>
will also spawn a thread for <code>Reactor</code> to run, as the IO events will need to be checked separately if
we have the message handling loop in the main thread.</p>
<p>The <code>Executor</code> also provides a function: <code>block_on</code>. The asynchronous function that's spawned by the <code>block_on</code> function
is just like the <code>main</code> function in a normal program: the runtime won't exit before this function ends without
further errors or interrupts occur during runtime.</p>
<p>The complete pseudocode for the <code>Executor</code>:</p>
<pre><code>Init():
    Initialize the Scheduler.
    Spawn a thread to init and run the Reactor.
    Set error hook for graceful shutdown to clean storages.
    Return instance of self

HandleRequests():
    while true:
        r &lt;- GetRequest()
        match r:
            Spawn(Task) -&gt; Put the Task to the execution system
            Shutdown -&gt; break

Run():
    HandleRequests()
    Send Shutdown message to the Reactor
    Join the thread for the Reactor

block_on(f):
    handle &lt;- spawn(f())
    Run()
    Get the result from handle
    Return result of f()
</code></pre>
<p>Next, we'll talk about the message passing in this design.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="passing-messages"><a class="header" href="#passing-messages">Passing messages</a></h1>
<p>According to the previous section, we have two commands to handle:</p>
<ol>
<li>spawn</li>
<li>shutdown</li>
</ol>
<p>The corresponding messages are defined:</p>
<pre><code class="language-rust">// in src/schedulers/mod.rs:
pub enum ScheduleMessage {
    Schedule(FutureIndex),
    Reschedule(FutureIndex),
    Shutdown,
}
</code></pre>
<p>Where <code>Schedule</code> is to handle spawn, and <code>Shutdown</code> is to handle shutdown.</p>
<p>The <code>Reschedule</code> message is an internal message that is used to re-queue a task, usually used when
a worker's queue is full that the worker can put excessive tasks back to global queue for others to take.
For more details, see <a href="layer/snd/../trd/schedule_procedure.html">The procedure of task scheduling</a>.</p>
<p>With the message defined, we can code the message handle loop of <code>Executor</code>:</p>
<pre><code class="language-rust">// Executor::run() code
fn run(mut self) {
    loop {
        // The sender half will send message.
        match self.scheduler.receiver().recv() { // Blocking receive
            Ok(msg) =&gt; match msg {
                ScheduleMessage::Schedule(future) =&gt; // Scheduler schedule future,
                ScheduleMessage::Reschedule(task) =&gt; // Scheduler reschedule task,
                ScheduleMessage::Shutdown =&gt; break,
            },
            Err(_) =&gt; { // sender disconnected
                log::debug!(&quot;exit...&quot;);
                break;
            }
        }
    }
    // Shutdown procedures...
}
</code></pre>
<p>The <code>Executor</code> will receive messages until:</p>
<ol>
<li>Sender half of the channel is disconnected.</li>
<li>Got <code>ScheduleMessage::Shutdown</code>.</li>
</ol>
<p>In the subsections, we'll look at:</p>
<ol>
<li>The sender half.</li>
<li>The message payload.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="spawner---message-sender"><a class="header" href="#spawner---message-sender">Spawner - message sender</a></h1>
<p>The sender half is defined as <code>Spawner</code>, which contains the sender half of the message passing channel.
When the <code>Executor</code> initialize, it will also initialize a global <code>Spawner</code> for user to use <code>spawn()</code> and <code>shutdown()</code>.</p>
<p>When calling <code>spawn()</code> or <code>Executor::block_on()</code>, the <code>Spawner</code> will do the following stuff before
<code>ScheduleMessage::Schedule</code> is sent:</p>
<ol>
<li>Create a shared memory for the future handle to use</li>
<li>Add extra code after the future need to spawned that
<ul>
<li>Put the execution result to the shared memory.</li>
<li>Check if it is called by <code>block_on</code>, if yes, shutdown the runtime.</li>
</ul>
</li>
<li>Allocate the <code>Future</code> in global <code>Future</code> object pool and get its key.</li>
<li>Construct the <code>FuturIndex</code> with the key and other payloads.</li>
<li>Send <code>ScheduleMessage::Schedule</code> with the <code>FutureIndex</code>.</li>
<li>Return the handle to the spawned future.</li>
</ol>
<p>The code:</p>
<pre><code class="language-rust">// Spawner::spawn_with_handle()
pub fn spawn_with_handle&lt;F&gt;(&amp;self, future: F, is_block: bool) -&gt; JoinHandle&lt;F::Output&gt;
where
    F: Future + Send + 'static,
    F::Output: Send + 'static,
{
    // Step 1.
    let result_arc: Arc&lt;Mutex&lt;Option&lt;F::Output&gt;&gt;&gt; = Arc::new(Mutex::new(None));
    // Step 2.
    let clone = result_arc.clone();
    let spawn_fut = async move {
        let output = future.await;
        // Store the result.
        let mut guard = clone.lock();
        guard.replace(output);
        // Check if the function is called by `Executor::block_on()`
        if is_block {
            log::info!(&quot;Shutting down...&quot;);
            shutdown();
        }
        Ok(())
    };
    // Step 3.
    let key = FUTURE_POOL
        .create_with(|seat| {
            seat.future.get_mut().replace(spawn_fut.boxed());
        })
        .unwrap();
    // Used in auto task yielding if enabled.
    let budget_index = BUDGET_SLAB
        .insert(AtomicUsize::new(DEFAULT_BUDGET))
        .unwrap();
    // Step 4., Step 5.
    self.tx
        .send(ScheduleMessage::Schedule(FutureIndex {
            key,
            budget_index,
            sleep_count: 0,
        }))
        .expect(&quot;Failed to send message&quot;);
    // Step 6.
    JoinHandle {
        spawn_id: key,
        registered: AtomicBool::new(false),
        inner: result_arc,
    }
}
</code></pre>
<p>For, <code>Shutdown</code> and <code>Reschedule</code>, the implementation is simple: simply send the corresponding message with necessary arguments.
We can also define <code>spawn()</code> and <code>shutdown()</code> now as their just a wrapper to call the global spawner and use its method call
to send messages to the <code>Executor</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="message-payload"><a class="header" href="#message-payload">Message payload</a></h1>
<p>The code in the previous section shows that the <code>FutureIndex</code> contains two extra payloads.
The two payloads are related to scheduling, that:</p>
<ol>
<li><code>sleep_count</code>: The sleep count is used in <code>HybridScheduler</code>, that it is used to prioritize the tasks. See <a href="layer/snd/../trd/hybrid.html">Hybrid Queue for Prioritized Work Stealing</a></li>
<li><code>budget_index</code>: Index for retrieving budgets, that is used for auto task yielding. See <a href="layer/snd/../trd/token_bucket.html">A token bucket like algorithm for auto task yielding</a></li>
</ol>
<p>Although the payloads are hard coded, when you design your own executor, you can store information that is used for
runtime metrics.</p>
<p>For generic payloads, you should make use of <code>Any</code> trait with <code>TypeId</code> to check whether the payload is the valid type.</p>
<p>Here is an example code:</p>
<pre><code class="language-rust">struct FutureIndex {
    key: usize,
    // essential fields across schedulers
    // ...

    // generic payload that needs down casting.
    payload: Arc&lt;Mutex&lt;Option&lt;Box&lt;dyn Any + Send&gt;&gt;&gt;&gt;,
}

// Impl this for some other struct and set Target to your payload type.
trait ExtractPayload {
    type Target: 'static;
    fn extract(index: &amp;FutureIndex) -&gt; Option&lt;Box&lt;Self::Target&gt;&gt; {
        let p = index.payload.lock().unwrap().take();
        match p {
            Some(payload) =&gt; {
                if payload.as_ref().type_id() == TypeId::of::&lt;Self::Target&gt;() {
                    Some(payload.downcast().unwrap())
                } else {
                    None
                }
            }
            None =&gt; None,
        }
    }
}
</code></pre>
<p>Next, we'll move on to <code>JoinHandle</code>'s design.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="join-handle-for-future"><a class="header" href="#join-handle-for-future">Join Handle for Future</a></h1>
<p>As shown before, we use a shared memory to store the result of the spawned future, then we put the
shared memory into <code>JoinHandle</code> and return to the user.</p>
<p>The <code>JoinHandle</code> is defined as:</p>
<pre><code class="language-rust">pub struct JoinHandle&lt;T&gt; {
    spawn_id: usize,
    registered: AtomicBool,
    inner: Arc&lt;Mutex&lt;Option&lt;T&gt;&gt;&gt;,
}
</code></pre>
<p>Where</p>
<ul>
<li><code>spawn_id</code> is the ID of the spawned future.</li>
<li><code>registered</code> is a flag to check if the waker of it is registered.</li>
<li><code>inner</code> is the shared memory.</li>
</ul>
<p>We can take a look at <code>join()</code> and <code>try_join()</code> with other structs and functions to see how it works:</p>
<pre><code class="language-rust">static JOIN_HANDLE_MAP: Lazy&lt;RwLock&lt;FxHashMap&lt;usize, Waker&gt;&gt;&gt; =
    Lazy::new(|| RwLock::new(FxHashMap::default()));

pub struct FutureJoin&lt;'a, T&gt; {
    handle: &amp;'a JoinHandle&lt;T&gt;,
}

impl&lt;T&gt; JoinHandle&lt;T&gt; {
    pub fn join(&amp;self) -&gt; FutureJoin&lt;'_, T&gt; {
        FutureJoin { handle: self }
    }
    pub fn try_join(&amp;self) -&gt; Option&lt;T&gt; {
        let mut guard = self.inner.lock();
        guard.take()
    }
    fn register_waker(&amp;self, waker: Waker) {
        JOIN_HANDLE_MAP.write().insert(self.spawn_id, waker);
        self.registered.store(true, Ordering::Relaxed);
    }
    fn deregister_waker(&amp;self) {
        JOIN_HANDLE_MAP.write().remove(&amp;self.spawn_id);
        self.registered.store(false, Ordering::Relaxed);
    }
}

impl&lt;'a, T&gt; Future for FutureJoin&lt;'a, T&gt; {
    type Output = T;
    fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt; {
        let me = self.handle;
        // Lock the shared memory and try to take the content of it.
        let mut guard = me.inner.lock();
        match guard.take() {
            // If success
            Some(val) =&gt; {
                // Deregister the waker from JOIN_HANDLE_MAP
                if me.registered.load(Ordering::Relaxed) {
                    me.deregister_waker();
                }
                // Return Ready(val)
                Poll::Ready(val)
            }
            // Else
            None =&gt; {
                // Register waker if the waker is not registered
                if !me.registered.load(Ordering::Relaxed) {
                    // spawned future can use it's own id to wake JoinHandle.
                    me.register_waker(cx.waker().clone());
                }
                // Return Pending
                Poll::Pending
            }
        }
    }
}

// Called when a spawned future finishes running
// If the waker is registered, we wake it up
// Otherwise, the JoinHandle hasn't request join.
pub(super) fn wake_join_handle(index: usize) {
    if let Some(waker) = JOIN_HANDLE_MAP.read().get(&amp;index) {
        waker.wake_by_ref();
    }
}
</code></pre>
<p>Since we use a shared memory that is <code>Arc&lt;Mutex&lt;Option&lt;T&gt;&gt;&gt;</code>, the <code>try_join()</code> can be implemented easily
by lock <code>inner</code> and take it. The <code>Option&lt;T&gt;</code>'s value can indicate the success or not.</p>
<p><code>join()</code> is a asynchronous function that will wait until the future of the handle finishes execution, that is,
if <code>inner</code> is <code>None</code>, we register the waker of the context where you call <code>join()</code> to a map.
The entry will use the spawned future's key (<code>spawn_id</code>) as its key so that the future can use the waker to wake
the context where <code>join()</code> is being called.</p>
<p>Take the following code as an example:</p>
<pre><code class="language-rust">async fn parent() {
    let handle = spawn(child());
    let result: T = handle.join().await; // waker is from parent's context
}

async fn child() -&gt; T {
    // do stuff...
    result
}
</code></pre>
<p><code>handle</code> will use the waker from context <code>cx</code> that is used to poll <code>parent</code>, while the waker registration uses
<code>child</code>'s index as its key so that <code>child</code> can wake <code>parent</code> to poll <code>handle.join()</code>.</p>
<p>Since we need to use the key of the spawned future as its key to the waker map, we can only use
a regular hash map with a <code>RwLock</code> so that we can specify its key. This is the simplest way I've
tried so far, otherwise we need to make more complicated code so that we don't need to use a hash map.</p>
<p>Anyway, that's all for the <code>Executor</code>, we'll now move one to the <code>Scheduler</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multi-thread-mania---scheduler"><a class="header" href="#multi-thread-mania---scheduler">Multi-thread mania - Scheduler</a></h1>
<p>In the previous sections, we've look into <code>Executor</code>'s design
with some key component remain unexplained.
One of them is the <code>Scheduler</code> trait.</p>
<p><code>Scheduler</code> is the part that is performing task scheduling and task execution.
In this part, we will discuss them in depth, including:</p>
<ol>
<li>The <code>Scheduler</code> trait.
<ul>
<li>Trait design</li>
<li>General <code>Worker</code> structure</li>
<li>Scheduling procedure</li>
</ul>
</li>
<li>Scheduling methods
<ul>
<li>Round Robin</li>
<li>Work Stealing</li>
<li>Hybrid Queue for Work Stealing</li>
</ul>
</li>
<li>Auto task yielding algorithm</li>
</ol>
<p>First, we will come to the trait itself...</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scheduler-trait-design"><a class="header" href="#scheduler-trait-design">Scheduler trait design</a></h1>
<p>To begin with, we'll list the key functions for a scheduler:</p>
<ol>
<li>Be able to interpret commands that is sent by <code>Executor</code>.</li>
<li>Spawn and handle worker threads.</li>
<li>Schedule tasks depending on the scheduling strategy.</li>
</ol>
<p>Since 3 is implementation dependent, our trait design will include the following
functions to fulfill the requirements in 1 and 2:</p>
<ul>
<li><code>init()</code>: The <code>init()</code> function should take an argument <code>size</code> that specifies the number of threads.
<ul>
<li>It will spawn the worker threads and set up a channel for message passing.</li>
<li>All internal channels and structures should be initialized in this function.</li>
<li>It will return an instance of itself and the <code>Spawner</code>, as mentioned before.</li>
</ul>
</li>
<li><code>schedule()</code>, <code>reschedule()</code>, <code>shutdown()</code>: The functions corresponding to <code>ScheduleMessage</code>.
<ul>
<li>Note that <code>shutdown()</code> should consume itself, so it will take <code>self</code> instead of <code>&amp;self</code> or <code>&amp;mut self</code>.</li>
</ul>
</li>
<li><code>receiver()</code>: Return the receiver half of the message passing.
<ul>
<li>The sender half is held by <code>Spawner</code> that is returned by <code>init()</code>.</li>
</ul>
</li>
</ul>
<h2 id="code"><a class="header" href="#code">Code</a></h2>
<p>With the requirement, we can define our trait like the following code:</p>
<pre><code class="language-rust">pub trait Scheduler {
    fn init(size: usize) -&gt; (Spawner, Self);
    fn schedule(&amp;mut self, index: FutureIndex);
    fn reschedule(&amp;mut self, index: FutureIndex);
    fn shutdown(self);
    fn receiver(&amp;self) -&gt; &amp;Receiver&lt;ScheduleMessage&gt;;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="general-worker-structure-and-logic"><a class="header" href="#general-worker-structure-and-logic">General Worker structure and logic</a></h1>
<p>Next we will define the worker's general structure and logic.
Since a worker's structure and behavior is implementation specific,
it's hard to use a trait and rule them all, but the general idea is shared: Receive and Execute.</p>
<p>The loop of processing can divide into 3 stages:</p>
<ol>
<li>Process all the tasks in local queue.</li>
<li>Non-blocking collect any of the woke up tasks. If there are any, push them into local queue and continue.</li>
<li>Wait for woke up tasks and new tasks. Push these tasks into local queue and continue.</li>
</ol>
<p>We can then turn it to a pseudo code:</p>
<pre><code>fn process():
    while true:
        if Some(t) &lt;- q.pop():
            t.run()
        else:
            cnt &lt;- 0
            while Ok(t) &lt;- wake_receiver.try_recv() or Ok(t) &lt;- new_task_receiver.try_recv():
                q.push(t)
                cnt &lt;- cnt + 1
            if cnt &gt; 0:
                continue
            if t &lt;- wake_receiver.recv() or t &lt;- new_task_receiver.recv():
                q.push(t)
                continue
            else if shutdown &lt;- notifier.recv():
                break
</code></pre>
<p>The code did the following stuff:</p>
<ol>
<li>If local queue contains some tasks, execute them.</li>
<li>Try to receive any scheduled or woke up tasks. If there any, push them and continue the loop.</li>
<li>Block receive any task that is scheduled or woke up. If there is one, push it and continue the loop.</li>
<li>If received a shutdown signal, break the process loop.</li>
</ol>
<p>The schedulers that I implemented myself follows this structure, with many specific stuff due to the scheduling method.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-procedure-of-task-scheduling"><a class="header" href="#the-procedure-of-task-scheduling">The procedure of task scheduling</a></h1>
<p>The procedure can be shown as a diagram:
<img src="layer/trd/../../assets/Schedule_Procedure_Bright.png" alt="Schedule Procedure" /></p>
<ol>
<li>The command sources will send command to the receiver.</li>
<li>The receiver will then handle the commands
<ul>
<li>If shutdown is received, run shutdown handler to shutdown the workers and join the threads.</li>
<li>Otherwise send the tasks to the injector.</li>
</ul>
</li>
<li>The injector will distributed the tasks to the workers to execute</li>
<li>The tasks that are woke up will be send back to the worker it was originally scheduled.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scheduling-method"><a class="header" href="#scheduling-method">Scheduling Method</a></h1>
<p>In this part, we'll implement 3 scheduling method with our defined <code>Scheduler</code> trait and <code>Worker</code> logic:</p>
<ol>
<li><code>RoundRobinScheduler</code></li>
<li><code>WorkStealingScheduler</code></li>
<li><code>HybridScheduler</code></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="round-robin"><a class="header" href="#round-robin">Round Robin</a></h1>
<p>This scheduler uses a Round-Robin style scheduling strategy.</p>
<h2 id="structs"><a class="header" href="#structs">Structs</a></h2>
<pre><code class="language-rust">enum Message {
    Close,
}

pub struct RoundRobinScheduler {
    size: usize,
    current_index: usize,
    threads: Vec&lt;(WorkerInfo, JoinHandle&lt;()&gt;)&gt;,
    rx: Receiver&lt;ScheduleMessage&gt;,
}

struct WorkerInfo {
    task_tx: Sender&lt;FutureIndex&gt;,
    tx: Sender&lt;Message&gt;,
}

struct Worker {
    _idx: usize,
    task_tx: Sender&lt;FutureIndex&gt;,
    task_rx: Receiver&lt;FutureIndex&gt;,
    rx: Receiver&lt;Message&gt;,
}
</code></pre>
<ul>
<li>The injector of this scheduler is replaced by an array of senders as we use Round-Robin.</li>
<li>Channels used:
<ul>
<li><code>task_tx</code>, <code>task_rx</code>: Channel for tasks. <code>task_tx</code> is copied to the waker and the threads array.
<ul>
<li>Because of this, woke up tasks and new tasks shared the same channel and the channel can be used as the task queue.</li>
</ul>
</li>
<li>Worker <code>tx</code>, <code>rx</code>: Shutdown notifier.</li>
<li>Scheduler <code>rx</code>: Commands receiver.</li>
</ul>
</li>
</ul>
<h2 id="scheduler-implementation"><a class="header" href="#scheduler-implementation">Scheduler implementation</a></h2>
<pre><code class="language-rust">impl Scheduler for RoundRobinScheduler {
    fn init(size: usize) -&gt; (Spawner, Self) {
        Self::new(size)
    }
    fn schedule(&amp;mut self, index: FutureIndex) {
        let worker_index = self.round();
        let task_tx = &amp;self.threads[worker_index].0.task_tx;
        task_tx.send(index).expect(&quot;Failed to send message&quot;);
    }
    fn reschedule(&amp;mut self, index: FutureIndex) {
        let worker_index = self.round();
        let task_tx = &amp;self.threads[worker_index].0.task_tx;
        task_tx.send(index).expect(&quot;Failed to send message&quot;);
    }
    fn shutdown(self) {
        for (info, handle) in self.threads {
            let tx = info.tx;
            tx.send(Message::Close).expect(&quot;Failed to send message&quot;);
            let _ = handle.join();
        }
    }
    fn receiver(&amp;self) -&gt; &amp;Receiver&lt;ScheduleMessage&gt; {
        &amp;self.rx
    }
}
</code></pre>
<p>The <code>schedule</code> and <code>reschedule</code> simply get an index by <code>round</code>, and use the channel corresponding to the index to schedule the task.
<code>shutdown</code> will simply use <code>tx</code> of every worker to send shutdown message.</p>
<h3 id="initialization"><a class="header" href="#initialization">Initialization</a></h3>
<pre><code class="language-rust">fn new(size: usize) -&gt; (Spawner, Self) {
    let (tx, rx) = flume::unbounded();
    let spawner = Spawner::new(tx);
    let threads: Vec&lt;(WorkerInfo, JoinHandle&lt;()&gt;)&gt; = (0..size)
        .map(|_idx| {
            let (tx, rx) = flume::unbounded();
            let (task_tx, task_rx) = flume::unbounded();
            let tx_clone = task_tx.clone();
            let handle = thread::spawn(move || {
                let worker = Worker {
                    _idx,
                    task_tx: tx_clone,
                    task_rx,
                    rx,
                };
                worker.run();
            });
            (WorkerInfo { task_tx, tx }, handle)
        })
        .collect();
    let scheduler = Self {
        size,
        current_index: 0,
        threads,
        rx,
    };
    (spawner, scheduler)
}
</code></pre>
<p>Simply create the global spawner and the worker threads with channels introduced previously.</p>
<h2 id="worker-run-loop"><a class="header" href="#worker-run-loop">Worker run loop</a></h2>
<pre><code class="language-rust">impl Worker {
    fn run(&amp;self) {
        loop {
            let exit_loop = Selector::new()
                .recv(&amp;self.task_rx, |result| match result {
                    Ok(index) =&gt; {
                        super::process_future(index, &amp;self.task_tx);
                        false
                    }
                    Err(_) =&gt; true,
                })
                .recv(&amp;self.rx, |result| match result {
                    Ok(Message::Close) =&gt; true,
                    Err(_) =&gt; true,
                })
                .wait();
            if exit_loop {
                break;
            }
        }
    }
}
</code></pre>
<p>Simply wait both <code>task_rx</code> and <code>rx</code>. If <code>task_rx</code> has tasks, receive it and run it.
If receive shutdown notification from <code>rx</code>, break the loop.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="work-stealing"><a class="header" href="#work-stealing">Work Stealing</a></h1>
<p>This scheduler implements the Work-Stealing strategy.</p>
<h2 id="structs-1"><a class="header" href="#structs-1">Structs</a></h2>
<pre><code class="language-rust">pub struct WorkStealingScheduler {
    _size: usize,
    _stealers: Vec&lt;Stealer&lt;FutureIndex&gt;&gt;,
    wait_group: WaitGroup,
    handles: Vec&lt;thread::JoinHandle&lt;()&gt;&gt;,
    // channels
    inject_sender: Sender&lt;FutureIndex&gt;,
    notifier: Broadcast&lt;Message&gt;,
    rx: Receiver&lt;ScheduleMessage&gt;,
}

struct TaskRunner {
    _idx: usize,
    worker: Ringbuf&lt;FutureIndex&gt;,
    stealers: Arc&lt;[Stealer&lt;FutureIndex&gt;]&gt;,
    // channels
    inject_receiver: Receiver&lt;FutureIndex&gt;,
    rx: Receiver&lt;Message&gt;,
    task_tx: Sender&lt;FutureIndex&gt;,
    task_rx: Receiver&lt;FutureIndex&gt;,
}

#[derive(Clone)]
enum Message {
    HaveTasks,
    Close,
}
</code></pre>
<ul>
<li>The injector is a channel used in SPMC condition.</li>
<li>The injector here will not perform any scheduling strategy (first comer takes first), as we use Work-Stealing strategy.</li>
<li>Channels used:
<ul>
<li><code>inject_sender</code>, <code>inject_receiver</code>: Injector.</li>
<li><code>notifier</code>, Worker <code>rx</code>: Broadcast <code>Message</code> to workers.
<ul>
<li>Either shutdown or wake the workers up to get new tasks</li>
</ul>
</li>
<li>Worker <code>task_tx</code>, <code>task_rx</code>: Channel for waking up tasks. <code>task_tx</code> is copied to the waker.</li>
</ul>
</li>
<li>Worker <code>worker</code> ring buffer is designed to have the Work-Stealing function.
<ul>
<li>It's capable of <code>push</code>, <code>pop</code>, and create a stealer for other workers to steal task from it.</li>
<li>The stealers are created on scheduler init and uses a <code>Arc</code> guarded slice for access.</li>
</ul>
</li>
<li>The <code>wait_group</code> is used to synchronize each Worker at shutdown phase.</li>
</ul>
<h2 id="scheduler-implementation-1"><a class="header" href="#scheduler-implementation-1">Scheduler implementation</a></h2>
<pre><code class="language-rust">impl Scheduler for WorkStealingScheduler {
    fn init(size: usize) -&gt; (Spawner, Self) {
        Self::new(size)
    }
    fn schedule(&amp;mut self, index: FutureIndex) {
        self.inject_sender
            .send(index)
            .expect(&quot;Failed to send message&quot;);
        self.notifier
            .broadcast(Message::HaveTasks)
            .expect(&quot;Failed to send message&quot;);
    }
    fn reschedule(&amp;mut self, index: FutureIndex) {
        self.inject_sender
            .send(index)
            .expect(&quot;Failed to send message&quot;);
        self.notifier
            .broadcast(Message::HaveTasks)
            .expect(&quot;Failed to send message&quot;);
    }
    fn shutdown(self) {
        self.notifier
            .broadcast(Message::Close)
            .expect(&quot;Failed to send message&quot;);
        log::debug!(&quot;Waiting runners to shutdown...&quot;);
        self.wait_group.wait();
        self.handles.into_iter().for_each(|h| h.join().unwrap());
        log::debug!(&quot;Shutdown complete.&quot;);
    }
    fn receiver(&amp;self) -&gt; &amp;Receiver&lt;ScheduleMessage&gt; {
        &amp;self.rx
    }
}
</code></pre>
<ul>
<li><code>schedule</code>, <code>reschedule</code>: First push the task to be scheduled into the injector channel, then use the notifier to broadcast <code>Message::HaveTasks</code> to workers.</li>
<li><code>shutdown</code>: First use <code>notifier</code> to broadcast <code>Message::Close</code>. The use <code>wait_group</code> to wait all the threads run to the point before exit, and then we join all the threads.</li>
</ul>
<h3 id="initialization-1"><a class="header" href="#initialization-1">Initialization</a></h3>
<pre><code class="language-rust">fn new(size: usize) -&gt; (Spawner, Self) {
    // 1.
    let mut _stealers: Vec&lt;Stealer&lt;FutureIndex&gt;&gt; = Vec::new();
    let stealers_arc: Arc&lt;[Stealer&lt;FutureIndex&gt;]&gt; = Arc::from(_stealers.as_slice());
    let (inject_sender, inject_receiver) = flume::unbounded();
    let mut handles = Vec::with_capacity(size);
    let (tx, rx) = flume::unbounded();
    let mut notifier = Broadcast::new();
    let spawner = Spawner::new(tx);
    let wait_group = WaitGroup::new();
    // 2.
    for _idx in 0..size {
        let worker = Ringbuf::new(4096);
        _stealers.push(worker.stealer());
        let ic = inject_receiver.clone();
        let sc = Arc::clone(&amp;stealers_arc);
        let wg = wait_group.clone();
        let rc = notifier.subscribe();
        let handle = thread::Builder::new()
            .name(format!(&quot;work_stealing_worker_{}&quot;, _idx))
            .spawn(move || {
                let (task_tx, task_rx) = flume::unbounded();
                let runner = TaskRunner {
                    _idx,
                    worker,
                    stealers: sc,
                    inject_receiver: ic,
                    rx: rc,
                    task_tx,
                    task_rx,
                };
                runner.run();
                log::debug!(&quot;Runner shutdown.&quot;);
                drop(wg);
            })
            .expect(&quot;Failed to spawn worker&quot;);
        handles.push(handle);
    }
    let scheduler = Self {
        _size: size,
        _stealers,
        wait_group,
        handles,
        inject_sender,
        notifier,
        rx,
    };
    (spawner, scheduler)
}
</code></pre>
<ol>
<li>Create global spawner, <code>wait_group</code>, <code>notifier</code>, injector channel, stealer array, and handlers array.</li>
<li>Create worker threads with a Work-Stealing ringbuf bounded with a size of 4096.</li>
</ol>
<h2 id="worker-run-loop-1"><a class="header" href="#worker-run-loop-1">Worker run loop</a></h2>
<pre><code class="language-rust">impl TaskRunner {
    fn run(&amp;self) {
        'outer: loop {
            // 1.
            if !self.worker.is_empty() {
                while let Some(index) = self.worker.pop() {
                    super::process_future(index, &amp;self.task_tx);
                }
            } else {
                log::debug!(&quot;Start collecting tasks...&quot;);
                // 2.
                let mut wakeup_count = 0;
                log::debug!(&quot;Collecting wokeups...&quot;);
                loop {
                    match self.task_rx.try_recv() {
                        Ok(index) =&gt; {
                            wakeup_count += 1;
                            if let Err(index) = self.worker.push(index) {
                                reschedule(index);
                            }
                        }
                        Err(TryRecvError::Empty) =&gt; break,
                        Err(TryRecvError::Disconnected) =&gt; break 'outer,
                    }
                }
                if wakeup_count &gt; 0 {
                    continue;
                }
                // 3.
                log::debug!(&quot;Try stealing tasks from other runners...&quot;);
                if let Ok(index) = self.inject_receiver.try_recv() {
                    if let Err(index) = self.worker.push(index) {
                        reschedule(index);
                    }
                    continue;
                }
                // 4.
                if let Some(index) = self.steal_others() {
                    if let Err(index) = self.worker.push(index) {
                        reschedule(index);
                    }
                    continue;
                }
                // Finally, wait for a single wakeup task or broadcast signal from scheduler
                log::debug!(&quot;Runner park.&quot;);
                // 5.
                let exit_loop = Selector::new()
                    .recv(&amp;self.task_rx, |result| match result {
                        Ok(index) =&gt; {
                            if let Err(index) = self.worker.push(index) {
                                reschedule(index);
                            }
                            false
                        }
                        Err(_) =&gt; true,
                    })
                    .recv(&amp;self.rx, |result| match result {
                        Ok(Message::HaveTasks) =&gt; false,
                        Ok(Message::Close) | Err(_) =&gt; true,
                    })
                    .wait();
                if exit_loop {
                    break 'outer;
                }
            }
        }
    }

    fn steal_others(&amp;self) -&gt; Option&lt;FutureIndex&gt; {
        self.stealers
            .iter()
            .map(|s| s.steal())
            .find(|s| s.is_success())
            .and_then(|s| s.success())
    }
}
</code></pre>
<ol>
<li>If <code>worker</code> is not empty, pop and process a task.</li>
<li>Non-blocking receive all tasks that is waked up.
<ul>
<li>If <code>wake_count &gt; 0</code>, continue the loop.</li>
</ul>
</li>
<li>Non-blocking receive a task that is queued in the injector.
<ul>
<li>If success, continue the loop.</li>
</ul>
</li>
<li>Try to steal a task using the stealers recorded in the <code>stealers</code> array.
<ul>
<li>If success, continue the loop.</li>
</ul>
</li>
<li>Block receive 1 woke up task or notifications that is <code>HaveTasks</code> or <code>Close</code>.
<ul>
<li><code>Close</code> indicates that it's time to shut down, break run loop.</li>
<li><code>HaveTasks</code> means there tasks scheduled/rescheduled. Continue the loop and run through step 3.</li>
</ul>
</li>
</ol>
<p>The <code>steal_others</code> make use of Rust's lazy iterator behavior, that it won't steal over 1 task, which is the first one that successes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hybrid-queue-for-prioritized-work-stealing"><a class="header" href="#hybrid-queue-for-prioritized-work-stealing">Hybrid Queue for Prioritized Work Stealing</a></h1>
<p>This section described a Hybrid queue used with the Work-Stealing strategy.</p>
<h2 id="why"><a class="header" href="#why">Why?</a></h2>
<p>Since the asynchronous runtime requires the task to be yielded if the I/O action
returns <code>EWOULDBLOCK</code>/<code>EAGAIN</code>, a task may be interrupt multiple times before
it actually progressed. This will happen if there are multiple tasks accessing
the same resource at the same time, some tasks may unfortunately be yielded
by the design way much more times than others.</p>
<p>To encounter this problem, an obvious solution is to use a priority queue and
set those with more yield count a higher priority to balance the possible
yield count.</p>
<p>But for me, a concurrent priority queue with a high performance is sadly
too hard for me to invent. With no other solutions available, I came out
an idea to use a composed data structure, just like using two stacks to
implement a queue.</p>
<p>The priority of a task is included in <code>FutureIndex</code>, called <code>sleep_count</code>.
<code>sleep_count</code> will be increased each time a related <code>Waker</code> is waked by the <code>Reactor</code>,
that is, the task had yielded for once. When tasks are loaded into the <code>hot</code> queue,
it will use this count as each task's priority.</p>
<h2 id="structure"><a class="header" href="#structure">Structure</a></h2>
<p>The structure and the functions can be described by the following diagram:
<img src="layer/trd/../../assets/Hybrid_Scheduler_Bright.png" alt="Hybrid Scheduler Queue" /></p>
<p>The details can be found in the texts of the diagram.</p>
<h2 id="modifications-from-workstealingscheduler"><a class="header" href="#modifications-from-workstealingscheduler">Modifications from <code>WorkStealingScheduler</code></a></h2>
<p>This scheduler works pretty much the same as the <code>WorkStealingScheduler</code>, with some modifications.</p>
<h3 id="1-worker-queue"><a class="header" href="#1-worker-queue">1. <code>worker</code> queue</a></h3>
<p>The <code>worker</code> queue's type is replaced by <code>TaskQueue</code> we defined:</p>
<pre><code class="language-rust">struct TaskQueue {
    cold: Ringbuf&lt;FutureIndex&gt;,
    hot: PriorityQueue&lt;FutureIndex, usize, BuildHasherDefault&lt;FxHasher&gt;&gt;,
}
</code></pre>
<h3 id="2-run-loop"><a class="header" href="#2-run-loop">2. Run loop</a></h3>
<p>Because the <code>worker</code> is now using <code>TaskQueue</code> as its queue, we need to modify the run loop to the following:</p>
<pre><code class="language-rust">impl TaskRunner {
    fn run(&amp;mut self) {
        'outer: loop {
            if let Some((index, _)) = self.queue.hot.pop() {
                super::process_future(index, &amp;self.task_wakeup_sender);
            } else {
                log::debug!(&quot;Start collecting tasks...&quot;);
                // Step 1. cold -&gt; hot
                log::debug!(&quot;Cold to hot&quot;);
                let mut push = false;
                if !self.queue.cold.is_empty() {
                    push = true;
                    // cold -&gt; hot
                    while let Some(index) = self.queue.cold.pop() {
                        self.queue.hot.push(index, index.sleep_count);
                    }
                }
                if push {
                    continue;
                }
                // Step 2. pull from wakeups
                log::debug!(&quot;Collecting wokeups...&quot;);
                let mut recv_count = 0;
                loop {
                    match self.task_wakeup_receiver.try_recv() {
                        Ok(index) =&gt; {
                            if let Err(index) = self.queue.cold.push(index) {
                                reschedule(index);
                            }
                            recv_count += 1;
                        }
                        Err(TryRecvError::Empty) =&gt; break,
                        Err(TryRecvError::Disconnected) =&gt; break 'outer,
                    }
                }
                if recv_count &gt; 0 {
                    // we aren't starving, no need to steal.
                    continue;
                }
                // Step 3. steal
                log::debug!(&quot;Try stealing tasks from other runners...&quot;);
                if let Ok(index) = self.inject_receiver.try_recv() {
                    if let Err(index) = self.queue.cold.push(index) {
                        reschedule(index);
                    }
                    continue;
                }
                if let Some(index) = self.steal_task() {
                    if let Err(index) = self.queue.cold.push(index) {
                        reschedule(index);
                    }
                    continue;
                }
                // Step 4. wait
                log::debug!(&quot;Runner park.&quot;);
                let exit_loop = Selector::new()
                    .recv(&amp;self.task_wakeup_receiver, |result| match result {
                        Ok(index) =&gt; {
                            if let Err(index) = self.queue.cold.push(index) {
                                reschedule(index);
                            }
                            false
                        }
                        Err(_) =&gt; true,
                    })
                    .recv(&amp;self.notify_receiver, |result| match result {
                        Ok(Message::HaveTasks) =&gt; {
                            if let Ok(index) = self.inject_receiver.try_recv() {
                                if let Err(index) = self.queue.cold.push(index) {
                                    reschedule(index);
                                }
                            }
                            false
                        }
                        Ok(Message::Close) | Err(_) =&gt; true,
                    })
                    .wait();
                if exit_loop {
                    break 'outer;
                }
            }
        }
    }
}
</code></pre>
<p>As you can see, we replace all <code>worker</code> actions with <code>queue.cold</code>,
and add a part to load task from <code>cold</code> to <code>hot</code> at the beginning in the <code>else</code> part.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="a-token-bucket-like-algorithm-for-auto-task-yielding"><a class="header" href="#a-token-bucket-like-algorithm-for-auto-task-yielding">A token bucket like algorithm for auto task yielding</a></h1>
<p>Reference: <a href="https://tokio.rs/blog/2020-04-preemption">Reducing tail latencies with automatic cooperative task yielding, <code>tokio</code></a></p>
<p>The problem described in <code>tokio</code>'s blog is reasonable, and I think that it needs to be
countered one day, so here is my approach to the solution - <code>BudgetFuture</code> trait.</p>
<h1 id="the-budgetfuture-trait"><a class="header" href="#the-budgetfuture-trait">The <code>BudgetFuture</code> trait</a></h1>
<p>First, we'll introduce its interface:</p>
<pre><code class="language-rust">pub trait BudgetFuture: FutureExt {
    fn poll(&amp;mut self, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt;
    where
        Self: Unpin,
    {
        poll_with_budget(self, cx)
    }
}

impl&lt;F: FutureExt + ?Sized&gt; BudgetFuture for F {}
</code></pre>
<p>The trait simply overloads the <code>poll</code> function with <code>poll_with_budget</code>, and its auto-implemented for
all types implementing <code>FutureExt</code>, which is auto-implemented for types implementing <code>Future</code>.</p>
<p>By this, if the user wants to enable this feature, the user can simply include this to use this feature.</p>
<p>Next, we'll look at the <code>poll_with_budget</code> function</p>
<h2 id="poll_with_budget-function"><a class="header" href="#poll_with_budget-function"><code>poll_with_budget</code> function</a></h2>
<p>The <code>poll_with_budget</code> function is defined with several helper functions, a slab, and some thread local storage global variables:</p>
<pre><code class="language-rust">static BUDGET_SLAB: Lazy&lt;Slab&lt;AtomicUsize&gt;&gt; = Lazy::new(Slab::new);
thread_local! {
    pub(crate) static USING_BUDGET: Cell&lt;bool&gt; = Cell::new(false);
    static CURRENT_INDEX: Cell&lt;usize&gt; = Cell::new(usize::MAX);
    // cache to reduce atomic actions
    static BUDGET_CACHE: Cell&lt;usize&gt; = Cell::new(usize::MAX);
}

pub(crate) fn budget_update(index: &amp;FutureIndex) -&gt; Option&lt;usize&gt; {
    let mut result = None;
    let budget_index = index.budget_index;
    let current_budget = match BUDGET_SLAB.get(budget_index) {
        Some(b) =&gt; b.load(Ordering::Relaxed),
        None =&gt; {
            let idx = BUDGET_SLAB
                .insert(AtomicUsize::new(DEFAULT_BUDGET))
                .expect(&quot;Slab is full!!!&quot;);
            result.replace(idx);
            DEFAULT_BUDGET
        }
    };
    let old_index = CURRENT_INDEX.with(|idx| idx.replace(budget_index));
    let old_budget = BUDGET_CACHE.with(|b| b.replace(current_budget));
    if old_index != usize::MAX &amp;&amp; old_budget != usize::MAX {
        if let Some(b) = BUDGET_SLAB.get(old_index) {
            b.store(old_budget, Ordering::Relaxed);
        }
    }
    result
}

pub(crate) fn poll_with_budget&lt;T, U&gt;(fut: &amp;mut T, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;U&gt;
where
    T: FutureExt&lt;Output = U&gt; + ?Sized + Unpin,
{
    USING_BUDGET.with(|x| x.replace(true));
    BUDGET_CACHE.with(|budget| {
        let val = budget.get();
        // if budget is zero, reschedule it by immediately wake the waker then return Poll::Pending (yield_now)
        if val == 0 {
            cx.waker().wake_by_ref();
            budget.set(DEFAULT_BUDGET);
            return Poll::Pending;
        }
        match fut.poll(cx) {
            Poll::Ready(x) =&gt; {
                // budget decreases when ready
                budget.set(val - 1);
                Poll::Ready(x)
            }
            Poll::Pending =&gt; Poll::Pending,
        }
    })
}

pub(crate) fn process_future(mut index: FutureIndex, tx: &amp;Sender&lt;FutureIndex&gt;) {
    if let Some(boxed) = FUTURE_POOL.get(index.key) {
        USING_BUDGET.with(|ub| {
            if ub.get() {
                if let Some(idx) = budget_update(&amp;index) {
                    index.budget_index = idx;
                }
                ub.replace(false);
            }
        });
        let finished = boxed.run(&amp;index, tx.clone());
        if finished {
            wake_join_handle(index.key);
            if !FUTURE_POOL.clear(index.key) {
                log::error!(
                    &quot;Failed to remove completed future with index = {} from pool.&quot;,
                    index.key
                );
            }
        }
    } else {
        log::error!(&quot;Future with index = {} is not in pool.&quot;, index.key);
    }
}
</code></pre>
<p>Let's crack this down...</p>
<h3 id="function-body"><a class="header" href="#function-body">Function body</a></h3>
<p>First, the <code>USE_BUDGET</code> thread local variable will be replaced to <code>true</code> to indicate that we're using budget.
Then we use the budget value stored in <code>BUDGET_CACHE</code>, which is visible throughout the thread and set by <code>budget_update</code>,
to poll the future. If a task spends all of its budgets, it will return <code>Pending</code> immediately with its budget restored to the default value.
We also wake the future up immediately such that it will re-enter the processing queue at the same time we return <code>Pending</code>.</p>
<h3 id="budget_update"><a class="header" href="#budget_update"><code>budget_update</code></a></h3>
<p>This function will be called every time a task is being processed by <code>process_future</code> if we're using budget.
The reason is that we'll cache a task's budget into thread local storage to lower the amount of atomic operation.
Whenever we switch to a new task, it will need to replace the cache with the current one in slab, and store the old one
back with atomic operation to update the information in slab.</p>
<p>The first time we call this function, the function will store a <code>AtomicUsize</code> inside the <code>BUDGET_SLAB</code>,
and return its index so the <code>budget_index</code> field will be a valid index.</p>
<h3 id="variables"><a class="header" href="#variables">Variables</a></h3>
<ul>
<li><code>USE_BUDGET</code>: TLS variable, indicate whether we're using budget.</li>
<li><code>CURRENT_INDEX</code>: TLS variable, cache of the budget's index for current task. Used to index the atomic variable for storing the updated cache value back to slab.</li>
<li><code>BUDGET_CACHE</code>: TLS variable, cache of the budget's value for current task. Any budget operations will first write this variable, then write back to the slab when <code>budget_update</code>.</li>
<li><code>BUDGET_SLAB</code>: Global variable, slab storage of all budgets. It will give an index for first-time budget usage, which is used to index the budget in the slab.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-io-event-harvester---reactor"><a class="header" href="#system-io-event-harvester---reactor">System IO Event Harvester - Reactor</a></h1>
<p>After all the stuff we talk about, we'll now talk about the <code>Reactor</code>, the main source for event.
Since you only need an event source to craft a reactor, it is possible that you can design a
non-IO asynchronous runtime in Rust. However, <code>my-async</code>'s main focus is building a IO runtime,
our reactor will stick to IO events.</p>
<p>In this section, we'll talk about:</p>
<ul>
<li><code>Reactor</code> design</li>
<li><code>Waker</code> management</li>
</ul>
<p>Let's get started...</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="io-event-registration"><a class="header" href="#io-event-registration">IO event registration</a></h1>
<h2 id="reactor-struct-definition"><a class="header" href="#reactor-struct-definition"><code>Reactor</code> struct definition</a></h2>
<p>Since our goal is to build an IO runtime, naturally we'll need to register IO
events we want to watch so that we can use an event based approach.</p>
<p>To achieve this, I use a library called <code>mio</code>, which is a library for non-blocking
system IO that wraps <code>epoll,</code> <code>IOCP,</code> <code>kqueue</code>, etc.</p>
<p>The library provides:</p>
<ul>
<li><code>Poll</code> for polling system IO events.
<ul>
<li>Note that this is different from the <code>Poll</code> returned by <code>Future::poll()</code></li>
</ul>
</li>
<li><code>Events</code> specifically for <code>Poll::wait()</code> to store returned events</li>
<li><code>Registry</code> that accept using reference to register events.</li>
<li><code>Token</code> that wraps a <code>usize</code> as event's token.</li>
</ul>
<p>We can then create a <code>Reactor</code> struct around <code>mio</code>'s structs:</p>
<pre><code class="language-rust">static REGISTRY: OnceCell&lt;Registry&gt; = OnceCell::new();
static POLL_WAKE_TOKEN: Token = Token(usize::MAX);
pub(super) static POLL_WAKER: OnceCell&lt;mio::Waker&gt; = OnceCell::new();

pub struct Reactor {
    poll: Poll,
    events: Events,
    extra_wakeups: Vec&lt;usize&gt;,
}
</code></pre>
<h3 id="note-2"><a class="header" href="#note-2">Note</a></h3>
<p><code>REGISTRY</code> is a global variable for global event register, since register only requires reference,
we can simply guard it with a one-time initialization atomic cell.</p>
<p><code>POLL_WAKER</code> and <code>POLL_WAKE_TOKEN</code> is used for shutdown signal. The waker is created by <code>mio::Waker::new()</code>.</p>
<p>The <code>extra_wakeups</code> is used to store event tokens that arrived before anyone await it.
This is required as <code>mio</code> uses edge-triggered mode for the underlying system IO mechanism, without
this you'll miss events under high loading scenarios.</p>
<h2 id="event-handling"><a class="header" href="#event-handling">Event handling</a></h2>
<p>Firstly, the code:</p>
<pre><code class="language-rust">static WAKER_SLAB: Lazy&lt;Slab&lt;Mutex&lt;Option&lt;Waker&gt;&gt;&gt;&gt; = Lazy::new(Slab::new);

impl Reactor {
    pub fn wait(&amp;mut self, timeout: Option&lt;Duration&gt;) -&gt; io::Result&lt;bool&gt; {
        self.poll.poll(&amp;mut self.events, timeout)?;
        if !self.events.is_empty() {
            log::debug!(&quot;Start process events.&quot;);
            for e in self.events.iter() {
                if e.token() == POLL_WAKE_TOKEN {
                    return Ok(true);
                }
                let idx = e.token().0;
                let waker_processed = process_waker(idx, |guard| {
                    if let Some(w) = guard.take() {
                        w.wake_by_ref();
                    }
                });

                if !waker_processed {
                    self.extra_wakeups.push(idx);
                }
            }
        }
        Ok(false)
    }
    pub fn check_extra_wakeups(&amp;mut self) -&gt; bool {
        let mut event_checked = false;
        self.extra_wakeups.retain(|&amp;idx| {
            let waker_processed = process_waker(idx, |guard| {
                if let Some(w) = guard.take() {
                    event_checked = true;
                    w.wake_by_ref();
                }
            });
            !waker_processed
        });
        event_checked
    }
}

fn process_waker&lt;F&gt;(idx: usize, f: F) -&gt; bool
where
    F: FnOnce(&amp;mut MutexGuard&lt;Option&lt;Waker&gt;&gt;),
{
    if is_registered(idx) {
        if let Some(mutex) = WAKER_SLAB.get(idx) {
            let mut guard = mutex.lock();
            f(&amp;mut guard);
            drop(guard);
            true
        } else {
            false
        }
    } else {
        false
    }
}

pub(crate) fn is_registered(token: usize) -&gt; bool {
    WAKER_SLAB.contains(token)
}

pub(crate) fn add_waker(token: usize, waker: Waker) -&gt; Option&lt;usize&gt; {
    let waker_found = process_waker(token, |guard| {
        if let Some(w) = guard.replace(waker.clone()) {
            w.wake_by_ref();
        }
    });
    if waker_found {
        None
    } else {
        WAKER_SLAB.insert(Mutex::new(Some(waker)))
    }
}

pub(crate) fn remove_waker(token: Token) -&gt; bool {
    WAKER_SLAB.remove(token.0)
}

pub fn register&lt;S&gt;(
    source: &amp;mut S,
    token: Token,
    interests: Interest,
    reregister: bool,
) -&gt; io::Result&lt;()&gt;
where
    S: Source + ?Sized,
{
    if let Some(registry) = REGISTRY.get() {
        if reregister {
            registry.reregister(source, token, interests)?;
        } else {
            registry.register(source, token, interests)?;
        }
    } else {
        log::error!(&quot;Registry hasn't initialized.&quot;)
    }
    Ok(())
}
</code></pre>
<ul>
<li><code>WAKER_SLAB</code> provides a global waker slab that will return an index to access the waker after insertion.
<ul>
<li>The returned index will be used as the event token that is registered with <code>REGISTERY</code>.</li>
<li>Since the same index may link to different wakers at different times, the entry is wrapped with <code>Mutex</code> to be able to replace contained <code>Waker</code>.</li>
</ul>
</li>
<li><code>process_waker</code> will check whether a <code>Waker</code> is present when an event occurs. If <code>WAKER_SLAB</code> doesn't contain it, or it's <code>None</code> at current state, mark return <code>false</code> to indicate that this is an extra wake-up that need to be handled after.</li>
<li><code>add_waker</code> will check whether a <code>Waker</code> exists in <code>WAKER_SLAB</code> with the token provide. If exists, swap the old one out and wake the old one, and return <code>None</code>. Otherwise, insert the waker and return a valid token for the caller to update.
<ul>
<li>By default, <code>IoWrapper</code> will use <code>usize::MAX</code> as token. This will insert the waker and update its token to a valid one, then it can use <code>register()</code> to register the event it needs with the updated token.</li>
</ul>
</li>
<li><code>check_extra_wakeups</code> will linearly check whether a <code>Waker</code> is currently present in <code>WAKER_SLAB</code> with scanned index in <code>extra_wakeups</code>.
<ul>
<li>This function will be called with <code>wait()</code> in a set to check if any of the events in <code>extra_wakeups</code> is needed after each <code>wait()</code>.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="poll-loop"><a class="header" href="#poll-loop">Poll loop</a></h1>
<p>After all the stuff defined, there's one more problem remain: Where to call <code>Reactor::wait()</code>?</p>
<ol>
<li>Use an isolated thread specifically for the <code>Reactor</code> to do its stuff</li>
<li>Use a <code>Mutex</code> and make it a global to let workers call it.</li>
</ol>
<p>I choose the first one since <code>Poll::wait()</code> requires <code>&amp;mut</code> unique reference to call, and <code>check_extra_wakeups</code>
will become a long critical section if there are loads of events that arrive before someone actually need it.
The reason that not using main thread to do this is because that the main thread will have <code>Executor</code>'s message
handling loop running.</p>
<p>In <code>Executor</code> initialization, we set up a <code>poll_thread</code>:</p>
<pre><code class="language-rust">// Executor::new()...
let poll_thread_handle = thread::Builder::new()
    .name(&quot;poll_thread&quot;.to_string())
    .spawn(move || Self::poll_thread())
    .expect(&quot;Failed to spawn poll_thread.&quot;);
// ...
fn poll_thread() {
    let mut reactor = reactor::Reactor::default();
    reactor.setup_registry();
    loop {
        // check if wakeups that is not used immediately is needed now.
        reactor.check_extra_wakeups();
        match reactor.wait(Some(Duration::from_millis(100))) {
            Ok(true) =&gt; break,
            Ok(false) =&gt; continue,
            Err(ref e) if e.kind() == io::ErrorKind::Interrupted =&gt; continue,
            Err(e) =&gt; {
                log::error!(&quot;reactor wait error: {}, exit poll thread&quot;, e);
                break;
            }
        }
    }
}
</code></pre>
<p>We use a waiting time of 100 ms to prevent the all-blocking scenario: Every thread is waiting on something and
not able to process new events.
This happens when it uses <code>reactor::wait(None)</code> for the <code>Reactor</code> to block until a readiness event comes in.
Because of this, I changed it to 100 ms waiting duration to make it not so active but keep running.
I'm still researching on how to deal with this, but for the current state, it works like a charm.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="load-balancing"><a class="header" href="#load-balancing">Load Balancing</a></h1>
<p>Load balancing is always a problem when it comes to multiple workers.
An efficient algorithm should:</p>
<ol>
<li>Distribute tasks evenly that the final elapsed time is reduced
as there are no long tails in terms of worker execution time.</li>
<li>Having a relatively small overhead that the algorithm won't
take a large portion when analyzing the final elapsed time.</li>
</ol>
<p>Currently, the <code>RoundRobinScheduler</code> uses Round-Robin, and <code>WorkStealingScheduler</code> and <code>HybridScheduler</code>
will let the worker who comes first to get the task from the global task queue as they use
work-stealing.</p>
<p>Further experiment needs to be conducted to find possibly better solution when it comes
to distributing tasks from the global task queue. If one is found, the runtime may generate
a better performance.</p>
<p>One possible direction is to estimate the execution time a spawned task by some
metrics and a heuristic function that can hopefully guesses the right worker to fill in.
This can take quite amount of time to assess and implement, and possibly have a worse performance
compare to current solutions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reactor-abstraction-for-different-systems"><a class="header" href="#reactor-abstraction-for-different-systems">Reactor abstraction for different systems</a></h1>
<p>Currently the <code>Reactor</code> is implemented with the <code>mio</code> library,
which supports system IO on major systems. We can extend the <code>Reactor</code>
by turning it into a trait that accepts different reactor implementation
just like the <code>Scheduler</code> trait.</p>
<p>Possible extra implementations are:</p>
<ul>
<li>Completion based system IO, such as <code>io_uring</code>.</li>
<li>Event through FFI or other schemes to bind existing systems.
<ul>
<li>One possible is that we can implement a reactor that can use results from JS promise under WASM environment.</li>
<li>Another one is that we can implement a reactor that can process XServer events through <code>xcb</code> that makes an async
X11 runtime which can be used to make async X11 applications with Rust.</li>
</ul>
</li>
</ul>
<p>All of this will need the <code>Reactor</code> to be abstracted into a trait before we attempt to develop such
reactors.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references"><a class="header" href="#references">References</a></h1>
<h2 id="main-references"><a class="header" href="#main-references">Main references</a></h2>
<p>Below is a list of articles and code that inspired me during the process of constructing
<code>my-async</code>:</p>
<ul>
<li>Asynchronous Programming in Rust: <a href="https://rust-lang.github.io/async-book/">https://rust-lang.github.io/async-book/</a></li>
<li><code>tokio</code>:
<ul>
<li>Their blog posts: <a href="https://tokio.rs/blog">https://tokio.rs/blog</a></li>
<li>Code: <a href="https://github.com/tokio-rs/tokio">https://github.com/tokio-rs/tokio</a></li>
</ul>
</li>
<li>Blog post: <a href="https://levelup.gitconnected.com/explained-how-does-async-work-in-rust-c406f411b2e2">https://levelup.gitconnected.com/explained-how-does-async-work-in-rust-c406f411b2e2</a>
<ul>
<li>This blog post has some nice explanation and graphs that give me the general idea of an asynchronous runtime
and helps me quickly construct my initial runtime structure.</li>
</ul>
</li>
</ul>
<h2 id="related-works-and-further-reading"><a class="header" href="#related-works-and-further-reading">Related works and Further reading</a></h2>
<p>Below is a list that I found worth reading when doing this project:</p>
<ul>
<li>
<p>The Node Experiment - Exploring Async Basics with Rust: <a href="https://cfsamson.github.io/book-exploring-async-basics/">https://cfsamson.github.io/book-exploring-async-basics/</a></p>
<ul>
<li>A piece of work that I found recently when finishing this document. It gives a great introduction to
knowledge that you might need to know (like the OS part that this document didn't cover).</li>
</ul>
</li>
<li>
<p><code>monoio</code> by bytedance, inc.: <a href="https://github.com/bytedance/monoio">https://github.com/bytedance/monoio</a></p>
<ul>
<li>Blog post about it, in a series of 5 posts: <a href="https://www.ihcblog.com/rust-runtime-design-1/">https://www.ihcblog.com/rust-runtime-design-1/</a> (Note that it's in Chinese)</li>
<li>A work that utilizes the GATs feature and uses <code>io-uring</code> as the underlying reactor. I find their
blog posts also explained a lot of details. The blog posts are in Simplified Chinese, though.
Prepare a translator if you can't read Chinese.</li>
</ul>
</li>
<li>
<p>Code of <code>tokio</code>, <code>async-std</code>, and <code>smol</code>: You probably can have an easier time when reading the code
of these bigger projects. The ultimate goal of <code>my-async</code> and this document
is to help you to understand how an asynchronous IO runtime can be implemented and build a picture
when reading the code.</p>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
